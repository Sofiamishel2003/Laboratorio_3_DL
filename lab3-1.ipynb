{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d120a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "689b71232e2b2445d032d7f28d8b8f3b",
     "grade": false,
     "grade_id": "cell-412524340acc422c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Laboratorio 3\n",
    "\n",
    "Sean bienvenidos de nuevo al laboratorio 3 de Deep Learning y Sistemas Inteligentes. Así como en los laboratorios pasados, espero que esta ejercitación les sirva para consolidar sus conocimientos en el tema de Redes Neuronales Recurrentes y LSTM.\n",
    "\n",
    "Este laboratorio consta de dos partes. En la primera trabajaremos una Red Neuronal Recurrente paso-a-paso. En la segunda fase, usaremos PyTorch para crear una nueva Red Neuronal pero con LSTM, con la finalidad de que no solo sepan que existe cierta función sino también entender qué hace en un poco más de detalle. \n",
    "\n",
    "Para este laboratorio estaremos usando una herramienta para Jupyter Notebooks que facilitará la calificación, no solo asegurándo que ustedes tengan una nota pronto sino también mostrandoles su nota final al terminar el laboratorio.\n",
    "\n",
    "Espero que esta vez si se muestren los *marks*. De nuevo me discupo si algo no sale bien, seguiremos mejorando conforme vayamos iterando. Siempre pido su comprensión y colaboración si algo no funciona como debería. \n",
    "\n",
    "Al igual que en el laboratorio pasado, estaremos usando la librería de Dr John Williamson et al de la University of Glasgow, además de ciertas piezas de código de Dr Bjorn Jensen de su curso de Introduction to Data Science and System de la University of Glasgow para la visualización de sus calificaciones. \n",
    "\n",
    "**NOTA:** Ahora tambien hay una tercera dependecia que se necesita instalar. Ver la celda de abajo por favor\n",
    "\n",
    "<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fa65b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:13.426912Z",
     "start_time": "2023-08-05T23:29:13.420034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/johnhw/jhwutils/zipball/master\n",
      "  Downloading https://github.com/johnhw/jhwutils/zipball/master\n",
      "     - 0 bytes ? 0:00:00\n",
      "     - 0 bytes ? 0:00:00\n",
      "     \\ 0 bytes ? 0:00:00\n",
      "     \\ 0 bytes ? 0:00:00\n",
      "     | 36.5 kB ? 0:00:00\n",
      "     | 36.5 kB ? 0:00:00\n",
      "     / 36.5 kB ? 0:00:00\n",
      "     / 46.8 kB 388.7 kB/s 0:00:00\n",
      "     - 46.8 kB 388.7 kB/s 0:00:00\n",
      "     - 46.8 kB 388.7 kB/s 0:00:00\n",
      "     \\ 46.8 kB 388.7 kB/s 0:00:00\n",
      "     \\ 46.8 kB 388.7 kB/s 0:00:00\n",
      "     | 46.8 kB 388.7 kB/s 0:00:00\n",
      "     / 46.8 kB 388.7 kB/s 0:00:00\n",
      "     / 57.0 kB 103.3 kB/s 0:00:00\n",
      "     - 57.0 kB 103.3 kB/s 0:00:00\n",
      "     - 75.4 kB 116.0 kB/s 0:00:00\n",
      "     \\ 75.4 kB 116.0 kB/s 0:00:00\n",
      "     \\ 75.4 kB 116.0 kB/s 0:00:00\n",
      "     | 89.2 kB 117.5 kB/s 0:00:00\n",
      "     | 89.2 kB 117.5 kB/s 0:00:00\n",
      "     / 89.2 kB 117.5 kB/s 0:00:01\n",
      "     - 89.2 kB 117.5 kB/s 0:00:01\n",
      "     - 112.1 kB 116.4 kB/s 0:00:01\n",
      "     \\ 112.1 kB 116.4 kB/s 0:00:01\n",
      "     \\ 119.1 kB 112.3 kB/s 0:00:01\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: jhwutils\n",
      "  Building wheel for jhwutils (pyproject.toml): started\n",
      "  Building wheel for jhwutils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jhwutils: filename=jhwutils-1.3-py3-none-any.whl size=41918 sha256=56b945f4bd72eeb93d423bf899ab3821c3a34f4a5ae6df7c2dafb1ec304d05cc\n",
      "  Stored in directory: C:\\Users\\JM\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-duh3rjd6\\wheels\\a8\\e7\\e3\\9542f8e4159ba644c6acd9f78babbe8489bb72667fb02ac54d\n",
      "Successfully built jhwutils\n",
      "Installing collected packages: jhwutils\n",
      "  Attempting uninstall: jhwutils\n",
      "    Found existing installation: jhwutils 1.3\n",
      "    Uninstalling jhwutils-1.3:\n",
      "      Successfully uninstalled jhwutils-1.3\n",
      "Successfully installed jhwutils-1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JM\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (1.16.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (3.5)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (11.3.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (2025.6.11)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (23.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\jm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-image) (0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JM\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/AlbertS789/lautils/zipball/master\n",
      "  Downloading https://github.com/AlbertS789/lautils/zipball/master\n",
      "     - 0 bytes ? 0:00:00\n",
      "     - 4.2 kB ? 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: lautils\n",
      "  Building wheel for lautils (pyproject.toml): started\n",
      "  Building wheel for lautils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for lautils: filename=lautils-1.0-py3-none-any.whl size=2890 sha256=d12149b41c65392d53c29d1d838673b1dcea5231104f43d6f1d8a9c59af84527\n",
      "  Stored in directory: C:\\Users\\JM\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-76nb3iob\\wheels\\1a\\50\\ba\\b3ceb937949f5894a896b68af5b5fdb598e50244141063e4db\n",
      "Successfully built lautils\n",
      "Installing collected packages: lautils\n",
      "Successfully installed lautils-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JM\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Una vez instalada la librería por favor, recuerden volverla a comentar.\n",
    "!pip install -U --force-reinstall --no-cache https://github.com/johnhw/jhwutils/zipball/master\n",
    "!pip install scikit-image\n",
    "!pip install -U --force-reinstall --no-cache https://github.com/AlbertS789/lautils/zipball/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d221fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.491024Z",
     "start_time": "2023-08-05T23:29:13.426912Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython import display\n",
    "from base64 import b64decode\n",
    "\n",
    "\n",
    "# Other imports\n",
    "from unittest.mock import patch\n",
    "from uuid import getnode as get_mac\n",
    "\n",
    "from jhwutils.checkarr import array_hash, check_hash, check_scalar, check_string, array_hash, _check_scalar\n",
    "import jhwutils.image_audio as ia\n",
    "import jhwutils.tick as tick\n",
    "from lautils.gradeutils import new_representation, hex_to_float, compare_numbers, compare_lists_by_percentage, calculate_coincidences_percentage\n",
    "\n",
    "###\n",
    "tick.reset_marks()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf165e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.506456Z",
     "start_time": "2023-08-05T23:29:14.491024Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a39756cb52fe963f67e015d4d8fe57a4",
     "grade": false,
     "grade_id": "cell-57de155e9f3409c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Seeds\n",
    "seed_ = 2023\n",
    "np.random.seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6688fc4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.522082Z",
     "start_time": "2023-08-05T23:29:14.506456Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "500bf8639033566b1f628a100f1180ca",
     "grade": true,
     "grade_id": "cell-e0ac5721852fe7fd",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Celda escondida para utlidades necesarias, por favor NO edite esta celda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff949f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T00:51:50.240511Z",
     "start_time": "2023-07-30T00:51:50.231535Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97d6b491fefaa9d0c1ffc3ac064a24bc",
     "grade": false,
     "grade_id": "cell-cdc148943062b4ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###### Información del estudiante en dos variables\n",
    "\n",
    "* carne_1 : un string con su carne (e.g. \"12281\"), debe ser de al menos 5 caracteres.\n",
    "* firma_mecanografiada_1: un string con su nombre (e.g. \"Albero Suriano\") que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)\n",
    "* carne_2 : un string con su carne (e.g. \"12281\"), debe ser de al menos 5 caracteres.\n",
    "* firma_mecanografiada_2: un string con su nombre (e.g. \"Albero Suriano\") que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18be1d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.537775Z",
     "start_time": "2023-08-05T23:29:14.522082Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cd4a99d7434f922d6754ac890fc97e5",
     "grade": false,
     "grade_id": "cell-1dec8918a2e1a2cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "carne_1 = \"22049\"\n",
    "firma_mecanografiada_1 = \"Sofía Mishell Velásquez\"\n",
    "carne_2 = \"22398\"\n",
    "firma_mecanografiada_2 = \"José Rodrigo Marchena\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d952cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.553326Z",
     "start_time": "2023-08-05T23:29:14.537775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"0\"}--> \n",
       "         ✓ [0 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"0\"}--> \n",
       "         ✓ [0 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deberia poder ver dos checkmarks verdes [0 marks], que indican que su información básica está OK \n",
    "\n",
    "with tick.marks(0): \n",
    "    assert(len(carne_1)>=5 and len(carne_2)>=5)\n",
    "\n",
    "with tick.marks(0):  \n",
    "    assert(len(firma_mecanografiada_1)>0 and len(firma_mecanografiada_2)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b6a3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc5db5a03eaba2adbf0f76c10e067442",
     "grade": false,
     "grade_id": "cell-3092f1f9ee984601",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Parte 1 - Construyendo una Red Neuronal Recurrente\n",
    "\n",
    "**Créditos:** La primera parte de este laboratorio está tomado y basado en uno de los laboratorios dados dentro del curso de \"Deep Learning\" de Jes Frellsen (DeepLearningDTU)\n",
    "\n",
    "La aplicación de los datos secuenciales pueden ir desde predicción del clima hasta trabajar con lenguaje natural. En este laboratorio daremos un vistazo a como las RNN pueden ser usadas dentro del modelaje del lenguaje, es decir, trataremos de predecir el siguiente token dada una secuencia. En el campo de NLP, un token puede ser un caracter o bien una palabra.\n",
    "\n",
    "### Representanción de Tokens o Texto\n",
    "\n",
    "Como bien hemos hablado varias veces, la computadora no entiende palabras ni mucho menos oraciones completas en la misma forma que nuestros cerebros lo hacen. Por ello, debemos encontrar alguna forma de representar palabras o caracteres en una manera que la computadora sea capaz de interpretarla, es decir, con números. Hay varias formas de representar un grupo de palabras de forma numérica, pero para fines de este laboratorio vamos a centrarnos en una manera común, llamada \"one-hot encoding\". \n",
    "\n",
    "#### One Hot Encoding\n",
    "Esta técnica debe resultarles familiar de cursos pasados, donde se tomaba una conjunto de categorías y se les asignaba una columna por categoría, entonces se coloca un 1 si el row que estamos evaluando es parte de esa categoría o un 0 en caso contrario. Este mismo acercamiento podemos tomarlo para representar conjuntos de palabras. Por ejemplo\n",
    "\n",
    "```\n",
    "casa = [1, 0, 0, ..., 0]\n",
    "perro = [0, 1, 0, ..., 0]\n",
    "```\n",
    "\n",
    "Representar un vocabulario grande con one-hot enconding, suele volverse ineficiente debido al tamaño de cada vector disperso. Para solventar esto, una práctica común es truncar el vocabulario para contener las palabras más utilizadas y representar el resto con un símbolo especial, UNK, para definir palabras \"desconocidas\" o \"sin importancia\". A menudo esto se hace que palabras tales como nombres se vean como UNK porque son raros.\n",
    "\n",
    "### Generando el Dataset a Usar\n",
    "\n",
    "Para este laboratorio usaremos un dataset simplificado, del cual debería ser más sencillo el aprender de él. Estaremos generando secuencias de la forma\n",
    "\n",
    "```\n",
    "a b EOS\n",
    "a a a a b b b b EOS\n",
    "```\n",
    "\n",
    "Noten la aparición del token \"EOS\", el cual es un caracter especial que denota el fin de la secuencia. Nuestro task en general será el predecir el siguiente token $t_n$, donde este podrá ser \"a\", \"b\", \"EOS\", o \"UNK\" dada una secuencia de forma ${t_1 , ... , t_{n-1}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cfc979b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.568947Z",
     "start_time": "2023-08-05T23:29:14.553326Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0adba37e43168d88355edd44ad433cb",
     "grade": false,
     "grade_id": "cell-62b6e4727b9bb25c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Una secuencia del grupo generado\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "# Reseed the cell\n",
    "np.random.seed(seed_)\n",
    "\n",
    "def generate_data(num_seq=100):\n",
    "    \"\"\"\n",
    "    Genera un grupo de secuencias, la cantidad de secuencias es dada por num_seq\n",
    "    \n",
    "    Args:\n",
    "    num_seq: El número de secuencias a ser generadas\n",
    "    \n",
    "    Returns:\n",
    "    Una lista de secuencias\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for i in range(num_seq):\n",
    "        # Genera una secuencia de largo aleatorio\n",
    "        num_tokens = np.random.randint(1,12) \n",
    "        # Genera la muestra\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        # Agregamos\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "sequences = generate_data()\n",
    "print(\"Una secuencia del grupo generado\")\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08cda1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a26144f688af47794960dfd5fdca804",
     "grade": false,
     "grade_id": "cell-844a1596734445c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Representación de tokens como índices\n",
    "\n",
    "En este paso haremos la parte del one-hot encoding. Para esto necesitaremos asignar a cada posible palabra de nuestro vocabulario un índice. Para esto crearemos dos diccionarios, uno que permitirá que dada una palabra nos dirá su representación como \"indice\" en el vocabulario, y el segundo que irá en dirección contraria. \n",
    "\n",
    "A estos les llamaremos `word_to_idx` y `idx_to_word`. La variable `vocab_size` nos dirá el máximo de tamaño de nuestro vocabulario. Si intentamos acceder a una palabra que no está en nuestro vocabulario, entonces se le reemplazará con el token \"UNK\" o su índice correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8940eccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.584569Z",
     "start_time": "2023-08-05T23:29:14.568947Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f52a0cd85402df075f20a68ae5f4e35",
     "grade": false,
     "grade_id": "cell-5276b445f04c739b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 100 secuencias y 4 tokens unicos incluyendo UNK\n",
      "El indice de 'b' es 1\n",
      "La palabra con indice 1 es b\n"
     ]
    }
   ],
   "source": [
    "def seqs_to_dicts(sequences):\n",
    "    \"\"\"\n",
    "    Crea word_to_idx y idx_to_word para una lista de secuencias\n",
    "    \n",
    "    Args:\n",
    "    sequences: lista de secuencias a usar\n",
    "    \n",
    "    Returns:\n",
    "    Diccionario de palabra a indice\n",
    "    Diccionario de indice a palabra\n",
    "    Int numero de secuencias\n",
    "    Int tamaño del vocabulario\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lambda para aplanar (flatten) una lista de listas\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # Aplanamos el dataset\n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Conteo de las ocurrencias de las palabras\n",
    "    word_count = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        word_count[word] += 1\n",
    "        \n",
    "    # Ordenar por frecuencia\n",
    "    word_count = sorted(list(word_count.items()), key=lambda x: -x[1])\n",
    "    \n",
    "    # Crear una lista de todas las palabras únicas\n",
    "    unique_words = [w[0] for w in word_count]\n",
    "    \n",
    "    # Agregamos UNK a la lista de palabras\n",
    "    unique_words.append(\"UNK\")\n",
    "    \n",
    "    # Conteo del número de secuencias y el número de palabras unicas\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "    \n",
    "    # Crear diccionarios mencionados\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "    \n",
    "    # Llenado de diccionarios\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        # Aprox 2 lineas para agregar\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "        \n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = seqs_to_dicts(sequences)\n",
    "\n",
    "print(f\"Tenemos {num_sequences} secuencias y {len(word_to_idx)} tokens unicos incluyendo UNK\")\n",
    "print(f\"El indice de 'b' es {word_to_idx['b']}\")\n",
    "print(f\"La palabra con indice 1 es {idx_to_word[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e17b1e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:14.616010Z",
     "start_time": "2023-08-05T23:29:14.584569Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e23613d7a17abd6db68772917d07f26d",
     "grade": true,
     "grade_id": "cell-c7aed80352919e68",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"2\"}--> \n",
       "         ✓ [2 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(3):        \n",
    "    assert(check_scalar(len(word_to_idx), '0xc51b9ba8'))\n",
    "    \n",
    "with tick.marks(2):        \n",
    "    assert(check_scalar(len(idx_to_word), '0xc51b9ba8'))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert(check_string(idx_to_word[0], '0xe8b7be43'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf7418",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41c936e1a2f35b960bd2e805e9634b6a",
     "grade": false,
     "grade_id": "cell-650d92ab739231c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Representación de tokens como índices\n",
    "\n",
    "Como bien sabemos, necesitamos crear nuestro dataset de forma que el se divida en inputs y targets para cada secuencia y luego particionar esto en training, validation y test (80%, 10%, 10%). Debido a que estamso haciendo prediccion de la siguiente palabra, nuestro target es el input movido (shifted) una palabra.\n",
    "\n",
    "Vamos a usar PyTorch solo para crear el dataset (como lo hicimos con las imagenes de perritos y gatitos de los laboratorios pasados). Aunque esta vez no haremos el dataloader. Recuerden que siempre es buena idea usar un DataLoader para obtener los datos de una forma eficienciente, al ser este un generador/iterador. Además, este nos sirve para obtener la información en batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d35905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.568534Z",
     "start_time": "2023-08-05T23:29:14.616010Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e41ed4ad2165904a221567eab31e222",
     "grade": false,
     "grade_id": "cell-186baacdbd91cc05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del training set 80\n",
      "Largo del validation set 10\n",
      "Largo del test set 10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    \n",
    "    # Definimos el tamaño de las particiones\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Dividir las secuencias en las particiones\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    # Funcion interna para obtener los targets de una secuencia\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Listas vacias\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Agregar informacion a las listas, ambas listas tienen L-1 palabras de una secuencia de largo L\n",
    "        # pero los targetes están movidos a la derecha por uno, para que podamos predecir la siguiente palabra\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Obtener inputs y targes para cada subgrupo\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Creación de datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "    \n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
    "\n",
    "print(f\"Largo del training set {len(training_set)}\")\n",
    "print(f\"Largo del validation set {len(validation_set)}\")\n",
    "print(f\"Largo del test set {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf12af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68e5ac2cd049c56737d20e23c06b751e",
     "grade": false,
     "grade_id": "cell-f048a8b17dec6268",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### One-Hot Encodings\n",
    "\n",
    "Ahora creemos una función simple para obtener la representación one-hot encoding de dado un índice de una palabra. Noten que el tamaño del one-hot encoding es igual a la del vocabulario. Adicionalmente definamos una función para encodear una secuencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50596f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.583462Z",
     "start_time": "2023-08-05T23:29:15.570496Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6eea35ea244f238189afef746c0c3067",
     "grade": false,
     "grade_id": "cell-91e0dff1547fcd06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodeado de 'a' con forma (4,)\n",
      "Encodeado de la secuencia 'a b' con forma (2, 4, 1).\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    Encodea una sola palabra dado su indice y el tamaño del vocabulario\n",
    "    \n",
    "    Args:\n",
    "     idx: indice de la palabra \n",
    "     vocab_size: tamaño del vocabulario\n",
    "    \n",
    "    Returns\n",
    "    np.array de lagro \"vocab_size\"\n",
    "    \"\"\"\n",
    "    # Init array encodeado\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Setamos el elemento a uno\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    Encodea una secuencia de palabras dado el tamaño del vocabulario\n",
    "    \n",
    "    Args:\n",
    "     sentence: una lista de palabras a encodear\n",
    "     vocab_size: tamaño del vocabulario\n",
    "     \n",
    "    Returns\n",
    "    np.array 3D de tamaño (numero de palabras, vocab_size, 1)\n",
    "    \"\"\"\n",
    "    # Encodear cada palabra en la secuencia\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Cambiar de forma para tener (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f\"Encodeado de 'a' con forma {test_word.shape}\")\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f\"Encodeado de la secuencia 'a b' con forma {test_sentence.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d828d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T04:31:37.634951Z",
     "start_time": "2023-07-30T04:31:37.621658Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "067f22f946e3fb34f11e220e7e8a387b",
     "grade": false,
     "grade_id": "cell-93bcd8db4fe6903f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora que ya tenemos lo necesario de data para empezar a trabajar, demos paso a hablar un poco más de las RNN\n",
    "\n",
    "## Redes Neuronales Recurrentes (RNN)\n",
    "\n",
    "Una red neuronal recurrente (RNN) es una red neuronal conocida por modelar de manera efectiva datos secuenciales como el lenguaje, el habla y las secuencias de proteínas. Procesa datos de manera cíclica, aplicando los mismos cálculos a cada elemento de una secuencia. Este enfoque cíclico permite que la red utilice cálculos anteriores como una forma de memoria, lo que ayuda a hacer predicciones para cálculos futuros. Para comprender mejor este concepto, consideren la siguiente imagen.\n",
    "\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230518134831/What-is-Recurrent-Neural-Network.webp\" alt=\"RNN\" />\n",
    "\n",
    "*Crédito de imagen al autor, imagen tomada de \"Introduction to Recurrent Neural Network\" de Aishwarya.27*\n",
    "\n",
    "Donde:\n",
    "* $x$ es la secuencia de input\n",
    "* $U$ es una matriz de pesos aplicada a una muestra de input dada\n",
    "* $V$ es una matriz de pesos usada para la computación recurrente para pasar la memroia en las secuencias\n",
    "* $W$ es una matriz de pesos usada para calcular la salida de cada paso\n",
    "* $h$ es el estado oculto (hidden state) (memoria de la red) para cada paso \n",
    "* $L$ es la salida resultante\n",
    "\n",
    "Cuando una red es extendida como se muestra, es más facil referirse a un paso $t$. Tenemos los siguientes calculos en la red\n",
    "\n",
    "* $h_t=f(U x_t + V h_{t-1}$ donde f es la función de activacion\n",
    "* $L_t = softmax(W h_t)$\n",
    "\n",
    "### Implementando una RNN\n",
    "\n",
    "Ahora pasaremos a inicializar nuestra RNN. Los pesos suelen inicializar de forma aleatoria, pero esta vez lo haremos de forma ortogonal para mejorar el rendimiento de nuestra red, y siguiendo las recomendaciones del paper dado abajo. \n",
    "\n",
    "Tenga cuidado al definir los elementos que se le piden, debido a que una mala dimensión causará que tenga resultados diferentes y errores al operar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83d6394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.599419Z",
     "start_time": "2023-08-05T23:29:15.584459Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aab983af86e5257de37bcca64632cee",
     "grade": false,
     "grade_id": "cell-8c9797de901a1f19",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed_)\n",
    "\n",
    "hidden_size = 50 # Numero de dimensiones en el hidden state\n",
    "vocab_size  = len(word_to_idx) # Tamaño del vocabulario\n",
    "\n",
    "def init_orthogonal(param):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters orthogonally.\n",
    "    Inicializa los pesos ortogonalmente\n",
    "    \n",
    "    Esta inicialización está dada por el siguiente paper:\n",
    "    https://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    \n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    \n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    \n",
    "    # Calcular factorización QR\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    \n",
    "    # Hacer Q uniforme de acuerdo a https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    \n",
    "    new_param = q\n",
    "    \n",
    "    return new_param\n",
    "\n",
    "\n",
    "def init_rnn(hidden_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Inicializa la RNN\n",
    "    \n",
    "    Args:\n",
    "     hidden_size:  Dimensiones del hidden state\n",
    "     vocab_size: Dimensión del vocabulario\n",
    "    \"\"\"\n",
    "    # Aprox 5 lineas para \n",
    "    # Definir la matriz de pesos (input del hidden state)\n",
    "    U = np.zeros((hidden_size, vocab_size))\n",
    "    # Definir la matriz de pesos de los calculos recurrentes\n",
    "    V = np.zeros((hidden_size, hidden_size))\n",
    "    # Definir la matriz de pesos del hidden state a la salida\n",
    "    W = np.zeros((vocab_size, hidden_size))\n",
    "    # Bias del hidden state\n",
    "    b_hidden = np.zeros((hidden_size, 1))\n",
    "    # Bias de la salida\n",
    "    b_out = np.zeros((vocab_size, 1))\n",
    "    # Para estas use np.zeros y asegurese de darle las dimensiones correcta a cada elemento\n",
    "    \n",
    "    # Aprox 3 lineas para inicializar los pesos de forma ortogonal usando la\n",
    "    # funcion init_orthogonal\n",
    "    U = init_orthogonal(U)\n",
    "    V = init_orthogonal(V)\n",
    "    W = init_orthogonal(W)\n",
    "    \n",
    "    # Return parameters as a tuple\n",
    "    return U, V, W, b_hidden, b_out\n",
    "\n",
    "\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "869c3065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.630336Z",
     "start_time": "2023-08-05T23:29:15.603408Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e16945840146775df25b57cf819b925",
     "grade": true,
     "grade_id": "cell-cebf0e26f26abbf2",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(5):        \n",
    "    assert check_hash(params[0], ((50, 4), 80.24369675632171))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[1], ((50, 50), 3333.838548574836))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[2], ((4, 50), -80.6410290517092))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[3], ((50, 1), 0.0))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[4], ((4, 1), 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6794e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3824d97af49f2479f2f568049ce82d01",
     "grade": false,
     "grade_id": "cell-0af0cee7ee982788",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Funciones de Activación\n",
    "A continuación definiremos las funciones de activación a usar, sigmoide, tanh y softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd9c9363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.646294Z",
     "start_time": "2023-08-05T23:29:15.631333Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8ce75b321c0cc6ca5c2e37786a296f6",
     "grade": false,
     "grade_id": "cell-cda959974e86198a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Calcula la función sigmoide para un array x\n",
    "\n",
    "    Args:\n",
    "     x: El array sobre el que trabajar\n",
    "     derivative: Si esta como verdadero, regresar el valor en la derivada\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12 #Evitar ceros\n",
    "    # Aprox 1 linea sobre x_safe para implementar la funcion\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    # Regresa la derivada de la funcion\n",
    "    if derivative: \n",
    "        return f * (1 - f)\n",
    "    # Regresa el valor para el paso forward\n",
    "    else: \n",
    "        return f\n",
    "    \n",
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Calcula la función tanh para un array x\n",
    "\n",
    "    Args:\n",
    "     x: El array sobre el que trabajar\n",
    "     derivative: Si esta como verdadero, regresar el valor en la derivada\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12 #Evitar ceros\n",
    "    # Aprox 1 linea sobre x_safe para implementar la funcion\n",
    "    f = np.tanh(x_safe)\n",
    "    \n",
    "    # Regresa la derivada de la funcion\n",
    "    if derivative: \n",
    "        return 1-f**2\n",
    "    # Regresa el valor para el paso forward\n",
    "    else: \n",
    "        return f\n",
    "    \n",
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Calcula la función softmax para un array x\n",
    "\n",
    "    Args:\n",
    "     x: El array sobre el que trabajar\n",
    "     derivative: Si esta como verdadero, regresar el valor en la derivada\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12 #Evitar ceros\n",
    "    # Aprox 1 linea sobre x_safe para implementar la funcion\n",
    "    exp_x = np.exp(x_safe)\n",
    "    f = exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "    # Regresa la derivada de la funcion\n",
    "    if derivative: \n",
    "        pass # No se necesita en backprog\n",
    "    # Regresa el valor para el paso forward\n",
    "    else: \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86e6f5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.662250Z",
     "start_time": "2023-08-05T23:29:15.647291Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88b15c243905bba412ed5b4ba65b5be0",
     "grade": true,
     "grade_id": "cell-a2ca064c7c460245",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(5):        \n",
    "    assert check_hash(sigmoid(params[0][0]), ((4,), 6.997641543410888))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(tanh(params[0][0]), ((4,), -0.007401604025076086))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(softmax(params[0][0]), ((4,), 3.504688021096135))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef9853",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d75af82e072ddb4a0c162e849158bcc1",
     "grade": false,
     "grade_id": "cell-f6476b1310ebea2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementación del paso Forward\n",
    "Ahora es el momento de implementar el paso forward usando lo que hemos implementado hasta ahora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bb06776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.677410Z",
     "start_time": "2023-08-05T23:29:15.663247Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65fdf4e2be5d9227b721ebfba3a76b88",
     "grade": false,
     "grade_id": "cell-d8f4885a4cccd525",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hidden_state, params):\n",
    "    \"\"\"\n",
    "    Calcula el paso forward de RNN\n",
    "    \n",
    "    Args:\n",
    "     inputs: Seccuencia de input a ser procesada\n",
    "     hidden_state: Un estado inicializado hidden state\n",
    "     params: Parametros de la RNN\n",
    "    \"\"\"\n",
    "    # Obtener los parametros\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    \n",
    "    # Crear una lista para guardar las salidas y los hidden states\n",
    "    outputs, hidden_states = [], []\n",
    "    \n",
    "    # Para cada elemento en la secuencia input\n",
    "    for t in range(len(inputs)):\n",
    "        x_t = inputs[t]\n",
    "        # Aprox 1 line para\n",
    "        # Calculo del nuevo hidden state usando tanh\n",
    "        # Recuerden que al ser el hidden state tienen que usar los pesos del input multiplicado por el input\n",
    "        #  a esto sumarle los pesos recurrentes por el hidden state y finalmente sumarle b\n",
    "        hidden_state = tanh(np.dot(U, x_t) + np.dot(V, hidden_state) + b_hidden)\n",
    "        # Aprox 1 linea\n",
    "        # para el calculo del output\n",
    "        # Al ser la salida, deben usar softmax sobre la multiplicación de pesos de salida con el hidden_state actual\n",
    "        #   es decir el calculado en el paso anterior y siempre sumarle su bias correspondiente\n",
    "        out = softmax(np.dot(W, hidden_state) + b_out)\n",
    "        # Guardamos los resultados y continuamos\n",
    "        outputs.append(out)\n",
    "        hidden_states.append(hidden_state.copy())\n",
    "    \n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c095221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.693405Z",
     "start_time": "2023-08-05T23:29:15.678377Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6b30539fff48162b40bf58b4d04a611",
     "grade": true,
     "grade_id": "cell-9db576244efaba24",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia Input:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Secuencia Target:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Secuencia Predicha:\n",
      "['a', 'b', 'a', 'a', 'a', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'b', 'b', 'b', 'b']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_input_sequence, test_target_sequence = training_set[0]\n",
    "\n",
    "# One-hot encode \n",
    "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size)\n",
    "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size)\n",
    "\n",
    "# Init hidden state con zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "outputs, hidden_states = forward_pass(test_input, hidden_state, params)\n",
    "\n",
    "print(\"Secuencia Input:\")\n",
    "print(test_input_sequence)\n",
    "\n",
    "print(\"Secuencia Target:\")\n",
    "print(test_target_sequence)\n",
    "\n",
    "print(\"Secuencia Predicha:\")\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "with tick.marks(5):        \n",
    "    assert check_hash(outputs, ((16, 4, 1), 519.7419046193046))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14fc0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c29e3a40c409913f6d3d0506d1b9d69f",
     "grade": false,
     "grade_id": "cell-8419bbbbfb1d7d89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementación del paso Backward\n",
    "\n",
    "Ahora es momento de implementar el paso backward.\n",
    "Si se pierden, remitanse a las ecuaciones e imagen dadas previamente.\n",
    "\n",
    "Usaremos una función auxiliar para evitar la explición del gradiente. Esta tecnica suele funcionar muy bien, si quieren leer más sobre esto pueden consultar estos enlances\n",
    "\n",
    "[Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)](https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\n",
    "\n",
    "[What exactly happens in gradient clipping by norm?](https://ai.stackexchange.com/questions/31991/what-exactly-happens-in-gradient-clipping-by-norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c404d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.709427Z",
     "start_time": "2023-08-05T23:29:15.694424Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7357465e411ae111b649d95e4fd7d6eb",
     "grade": false,
     "grade_id": "cell-9c36e2544990bfd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clipea (recorta?) el gradiente para tener una norma máxima de `max_norm`\n",
    "    Esto ayudará a prevenir el problema de la gradiente explosiva (BOOM!)\n",
    "    \"\"\" \n",
    "    # Setea el máximo de la norma para que sea flotante\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculamos la norma L2 al cuadrado para cada gradiente y agregamos estas a la norma total\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    # Cuadrado de la normal total\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculamos el coeficiente de recorte\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # Si el total de la norma es más grande que el máximo permitido, se recorta la gradiente\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    return grads\n",
    "\n",
    "\n",
    "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
    "    \"\"\"\n",
    "    Calcula el paso backward de la RNN\n",
    "    \n",
    "    Args:\n",
    "     inputs: secuencia de input\n",
    "     outputs: secuencia de output del forward\n",
    "     hidden_states: secuencia de los hidden_state del forward\n",
    "     targets: secuencia target\n",
    "     params: parametros de la RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener los parametros\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    \n",
    "    # Inicializamos las gradientes como cero (Noten que lo hacemos para los pesos y bias)\n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "    \n",
    "    # Llevar el record de las derivadas de los hidden state y las perdidas (loss)\n",
    "    d_h_next = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    \n",
    "    # Iteramos para cada elemento en la secuencia output\n",
    "    # NB: Iteramos de regreso sobre t=N hasta 0\n",
    "    for t in reversed(range(len(outputs))):\n",
    "\n",
    "        # Aprox 1 linea para calcular la perdida cross-entry (un escalar)\n",
    "        # Hint: Sumen +1e-12 a cada output_t\n",
    "        # Hint2: Recuerden que la perdida es el promedio de multiplicar el logaritmo de los output con los targets\n",
    "        loss += -np.sum(targets[t] * np.log(outputs[t] + 1e-12)) / targets[t].shape[0]\n",
    "\n",
    "        d_o = outputs[t].copy()\n",
    "        # Aprox 1 linea para backpropagate en los output (derivada del cross-entropy)\n",
    "        # Si se sienten perdidos refieran a esta lectura: http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "        d_o -= targets[t] \n",
    "        # Aprox 1 lineas para hacer el backpropagation de W\n",
    "        d_W += np.dot(d_o, hidden_states[t].T)\n",
    "        d_b_out += d_o\n",
    "        \n",
    "        # Aprox 1 linea para hacer el backprop de h\n",
    "        d_h = np.dot(W.T, d_o) + d_h_next\n",
    "        # Hint: Probablemente necesiten sacar la transpuesta de W\n",
    "        # Hint2: Recuerden sumar el bias correcto!\n",
    "\n",
    "        # Aprox 1 linea para calcular el backprop en la funcion de activacion tanh\n",
    "        d_f = tanh(hidden_states[t], derivative=True) * d_h\n",
    "        # Hint: Recuerden pasar el parametro derivate=True a la funcion que definimos\n",
    "        # Hint2: Deben multiplicar con d_h\n",
    "        d_b_hidden += d_f\n",
    "        \n",
    "        # Aprox 1 linea para backprop en U\n",
    "        d_U += np.dot(d_f, inputs[t].T)\n",
    "        \n",
    "        # Aprox 1 linea para backprop V\n",
    "        d_V += np.dot(d_f, hidden_states[t-1].T)\n",
    "        d_h_next = np.dot(V.T, d_f)\n",
    "    \n",
    "    # Empaquetar las gradientes\n",
    "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
    "    \n",
    "    # Corte de gradientes\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b7b1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.724695Z",
     "start_time": "2023-08-05T23:29:15.710439Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e114a2a7bf6752fd90bf75a740001356",
     "grade": true,
     "grade_id": "cell-65758aa67361b673",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss, grads = backward_pass(test_input, outputs, hidden_states, test_target, params)\n",
    "\n",
    "with tick.marks(5):        \n",
    "    assert check_scalar(loss, '0xf0c8ccc9')\n",
    "\n",
    "with tick.marks(5):        \n",
    "    assert check_hash(grads[0], ((50, 4), -16.16536590645467))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(grads[1], ((50, 50), -155.12594909703253))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(grads[2], ((4, 50), 1.5957812992239038))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b9abc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "853d497293018f4e60eeaf31fa548bfd",
     "grade": false,
     "grade_id": "cell-06bca206671d7909",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Optimización\n",
    "\n",
    "Considerando que ya tenemos el paso forward y podemos calcular gradientes con el backpropagation, ya podemos pasar a entrenar nuestra red. Para esto necesitaremos un optimizador. Una forma común y sencilla es implementar la gradiente descediente. Recuerden la regla de optimizacion\n",
    "$$\n",
    "θ = θ - α * ∇J(θ)\n",
    "$$\n",
    "\n",
    "* $θ$ son los parametros del modelo\n",
    "* $α$ es el learning rate\n",
    "* $∇J(θ)$ representa la gradiente del costo J con respecto de los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d0649f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:29:15.740315Z",
     "start_time": "2023-08-05T23:29:15.726168Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a816758f7791729583e774286d7ab13f",
     "grade": false,
     "grade_id": "cell-54add6e82ed32f01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Iteramos sobre los parametros y las gradientes\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f024f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09c0aab76534abb28f1e0fa5f0bbd13c",
     "grade": false,
     "grade_id": "cell-52ac5cccec0e2107",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Entrenamiento\n",
    "Debemos establecer un ciclo de entrenamiento completo que involucre un paso forward, un paso backprop, un paso de optimización y validación. Se espera que el proceso de training dure aproximadamente 5 minutos (o menos), lo que le brinda la oportunidad de continuar leyendo mientras se ejecuta&#x1F61C;\t\n",
    "\n",
    "Noten que estaremos viendo la perdida en el de validación (no en el de testing) esto se suele hacer para ir observando que tan bien va comportandose el modelo en terminos de generalización. Muchas veces es más recomendable ir viendo como evoluciona la métrica de desempeño principal (accuracy, recall, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1db77ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.382237Z",
     "start_time": "2023-08-05T23:29:15.741282Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e207987552b230e721859e0270e1ad61",
     "grade": false,
     "grade_id": "cell-e184f5f494d827a1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 0, training loss: 3.5035200857391224, validation loss: 4.187614537572069\n",
      "Epoca 100, training loss: 2.364151410878283, validation loss: 2.83930206647734\n",
      "Epoca 200, training loss: 1.9542781384482197, validation loss: 2.33815929778157\n",
      "Epoca 300, training loss: 1.7350494526120606, validation loss: 2.0925688632889856\n",
      "Epoca 400, training loss: 1.6162232494669708, validation loss: 1.979044587599431\n",
      "Epoca 500, training loss: 1.5504957912139017, validation loss: 1.9331494811503618\n",
      "Epoca 600, training loss: 1.510179238460863, validation loss: 1.9168234894230562\n",
      "Epoca 700, training loss: 1.4813212404502065, validation loss: 1.9125961996120058\n",
      "Epoca 800, training loss: 1.457619495806245, validation loss: 1.9148388299555816\n",
      "Epoca 900, training loss: 1.4354786197331075, validation loss: 1.9212055719855161\n",
      "Epoca 1000, training loss: 1.4100503405101503, validation loss: 1.9244759118523855\n",
      "Epoca 1100, training loss: 1.3724736522583654, validation loss: 1.904517101126857\n",
      "Epoca 1200, training loss: 1.3197845075914167, validation loss: 1.8477555238223196\n",
      "Epoca 1300, training loss: 1.2780915705632077, validation loss: 1.8016285539954358\n",
      "Epoca 1400, training loss: 1.233211937153443, validation loss: 1.7244012451654434\n",
      "Epoca 1500, training loss: 1.197803235591056, validation loss: 1.6791058023321674\n",
      "Epoca 1600, training loss: 1.1871474206401276, validation loss: 1.693021123447545\n",
      "Epoca 1700, training loss: 1.1845563568456132, validation loss: 1.717263863878068\n",
      "Epoca 1800, training loss: 1.1869187349271835, validation loss: 1.7460496906432463\n",
      "Epoca 1900, training loss: 1.1926770755709533, validation loss: 1.7769825394744836\n"
     ]
    }
   ],
   "source": [
    "# Hyper parametro\n",
    "# Se coloca como \"repsuesta\" para que la herramienta no modifique el numero de iteraciones que colocaron \n",
    "num_epochs = 2000\n",
    "\n",
    "\n",
    "# Init una nueva RNN\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "\n",
    "# Init hiddent state con ceros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Rastreo de perdida (loss) para training y validacion\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Iteramos para cada epoca\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Perdidas en zero\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    # Para cada secuencia en el grupo de validación\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode el input y el target\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        \n",
    "        # Re-init el hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Aprox 1 line para el paso forward \n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "\n",
    "        # Aprox 1 line para el paso backward\n",
    "        loss, _ = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)\n",
    "        \n",
    "        # Actualización de perdida\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode el input y el target\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        \n",
    "        # Re-init el hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Aprox 1 line para el paso forward \n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "        # Aprox 1 line para el paso backward\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)\n",
    "\n",
    "        # Validar si la perdida es nan, llegamos al problema del vanishing gradient POOF! \n",
    "        if np.isnan(loss):\n",
    "            raise ValueError(\"La gradiente se desvanecio... POOF!\")\n",
    "        \n",
    "        # Actualización de parámetros\n",
    "        params = update_parameters(params, grads, lr=3e-4)\n",
    "        \n",
    "        # Actualización de perdida\n",
    "        epoch_training_loss += loss\n",
    "        \n",
    "    # Guardar la perdida para graficar\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Mostrar la perdida cada 100 epocas\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoca {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "37307e6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.538285Z",
     "start_time": "2023-08-05T23:32:05.382237Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c657c86ccab3ced18f8a9604bade0e2",
     "grade": true,
     "grade_id": "cell-67387da31438dd57",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia Input:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Secuencia Target:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Secuencia Predicha:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS', 'EOS', 'EOS']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABRqElEQVR4nO3dB5hU5f328R+9V5HeQWkC0pSigIjSYiB2JAoWDAiWaIxBTVDyKih/xRJFTcQSVKxgQVCkWSjSm4giValKWXqd97rPydmdXXZnd9mpZ76f63quM21nztmZnXPvU/MFAoGAAQAA+ET+WO8AAABAOBFuAACArxBuAACArxBuAACArxBuAACArxBuAACArxBuAACArxS0JHPy5EnbsmWLlSpVyvLlyxfr3QEAADmgafn27dtnVatWtfz5Q9fNJF24UbCpUaNGrHcDAACchs2bN1v16tVDPibpwo1qbLxfTunSpWO9OwAAIAdSUlKcygnvPB5K0oUbrylKwYZwAwBAYslJlxI6FAMAAF8h3AAAAF8h3AAAAF9Juj43AIDwOnHihB07dizWuwEfKFy4cLbDvHOCcAMAOO15R7Zt22Z79uyJ9a7AJ/Lnz2916tRxQk5eEG4AAKfFCzYVK1a04sWLMzEqwjLJ7tatW61mzZp5+jwRbgAAp9UU5QWbM844I9a7A58488wznYBz/PhxK1So0Gk/Dx2KAQC55vWxUY0NEC5ec5TCc14QbgAAp42mKMTj54lwAwAAfIVwAwAAfIVwAwBAHtWuXdueeuqpHD9+1qxZThNMpIfRv/rqq1a2bFlLNoSbMFHfp23bzNaujfWeAACyokARqjz00EOn9bwLFiywW2+9NcePb9++vTPkuUyZMqf1egiNoeBhMmOG2aWXmjVpYrZyZaz3BgCQGQUKz9tvv23/+Mc/bM2aNam3lSxZMt0khRq1U7BgwRwNYc7tqKDKlSvn6meQc9TchEmVKu426O8GAJJLIGB24EBsil47BxQovKJaE9XWeNe///57K1WqlE2ZMsVatWplRYoUsa+//tp++ukn6927t1WqVMkJP23atLEvvvgiZLOUnvc///mP/eEPf3CGy5911ln20UcfZdks5TUfffbZZ9aoUSPndbp3754ujGnulzvuuMN5nOYWuu+++6x///7Wp0+fXL1NY8eOtXr16jkBq0GDBvbf//436C0MOLVXmkRPx1+1alXnNT3PP/+8cyxFixZ1fh9XXnmlxSPCTZhUrepud+0yO3Ik1nsDADFw8KCqPmJT9Nph8re//c1GjRplq1evtmbNmtn+/futZ8+eNn36dFuyZIkTOi677DLbtGlTyOd5+OGH7eqrr7bly5c7P9+vXz/bpZNElr++g/Z///d/Ttj48ssvnef/y1/+knr/Y489Zm+88Ya98sor9s0331hKSopNmjQpV8c2ceJEu/POO+2ee+6xlStX2p/+9Ce78cYbbebMmc7977//vo0ZM8ZefPFF+/HHH53nb9q0qXPfwoULnaAzYsQIp7Zr6tSp1rFjR4tLgSSzd+9exXtnG04nTwYCRYroX4dAYP36sD41AMSdQ4cOBb777jtnm2r/fvdLMBZFr51Lr7zySqBMmTKp12fOnOmcHyZNmpTtzzZp0iTw7LPPpl6vVatWYMyYManX9TwPPvhg0K9mv3PblClT0r3W7t27U/dF19euXZv6M88991ygUqVKqdd1efTo0anXjx8/HqhZs2agd+/eOT7G9u3bBwYOHJjuMVdddVWgZ8+ezuUnnngicPbZZweOHj16ynO9//77gdKlSwdSUlICUf1cncb5m5qbMNG8Q17zKU1TAJKSZivevz82JYwzJbdu3TrdddXcqAZFzUVqElKTkWp1squ5Ua2Pp0SJEla6dGnbsWNHlo9X85WaizxVqlRJffzevXtt+/btdt5556XeX6BAAaf5LDdWr15tHTp0SHebrut2ueqqq+zQoUNWt25dGzhwoFPTo+YwueSSS6xWrVrOfddff71Ti6TapnhEuIlA0xThBkDS/pdXokRsShhnSlYQCaZgo5P8o48+al999ZUtXbrUaao5evRoyOfJuDaS+thoccjcPN6tBIqeGjVqOE1O6ltTrFgxu+2225ymJy23of5IixcvtrfeessJXuqM3bx587hcFZ5wE0Z0KgYA/1H/lgEDBjidgxVq1Pl4w4YNUd0HdX5WB14NOfdoJJfCRm40atTIOZ5gut64cePU6wo16lP0zDPPOB2f586daytWrHDu08ixrl272uOPP+70JdLvYYaGC8cZhoJHINxs2RLrPQEAhItGB33wwQfOCV+1KX//+99D1sBEyu23324jR460+vXrW8OGDe3ZZ5+13bt352o9pnvvvdfp5NyiRQsnpHz88cfOsXmjvzRqS6Hp/PPPd5rJxo8f74QdNUd98skntm7dOqcmp1y5cvbpp586vweNuIo3hJswouYGAPznySeftJtuusmZeK9ChQrOEGyNVIo2ve62bdvshhtucPrbaNLAbt26OZdzqk+fPvb00087o7I0aqpOnTrO6KvOnTs796tPkUaK3X333U7IUU2VApCGnus+BSENFT98+LAT+tRE1UQTvMWZfOpVbElEH0hV76lzljp3hdMrr5jddJNZ9+5mU6aE9akBIK7o5LZ+/Xrn5Kg5TxB9qjVRM5NqYv75z3+a3z9XKbk4f1NzE0Y0SwEAImXjxo32+eefW6dOnezIkSP2r3/9ywkC1113Xax3Le7QoTiMaJYCAERK/vz5nT4xmiFZw7fVyVd9ZVR7g/SouYnAUPCdO82OHdOwvljvEQDALzRMO+NIJ2SOmpswOuMMDZNzL2/fHuu9AQAgORFuwih//rRZiul3AwBAbBBuwox+NwAAxBbhJsxYggEAgNgi3IQZNTcAAMRW3IQbzYioKaTvuuuukI979913nWmnNbmPZk7U9M/xhLluAMD/NKNv8Pmqdu3a9tRTT4X8GZ3jJk2alOfXDtfzhKJZiM8991xLVHERbrQQ2IsvvphuefjMzJkzx/r27Ws333yzLVmyxJlGWmXlypUWL2iWAoD4pfWhumsa+UxoxW8FBy0IeTrnMS2HEI2AsXXrVuvRo0dYX8tvYh5u9u/fb/369bN///vfzkJcoWg9DH0otfCXJi3SdNMtW7Z0ZmmMFzRLAUD80j/H06ZNs59//vmU+7TGUuvWrbP9RzszZ555prPQZDRoVfIiRYpE5bUSVczDzZAhQ6xXr17O6qTZ0bLrGR+nRcN0e1Y0RbXWowgukUS4AYD49bvf/c4JIprpN+M/2ur2oPDz22+/Oa0E1apVcwKLukBogchQMjZL/fjjj87q2epC0bhxYydQZbYQ5tlnn+28Rt26dZ3Vxo9pBtj/rc798MMP27Jly5zaJBVvnzM2S2mm4i5dujird2uBy1tvvdU5Hs+AAQOcVg4tllmlShXnMTr3eq+V03WsRowYYdWrV3eClWqUpk6dmnr/0aNHbejQoc7z65i1irhWMBctYalaqJo1azo/W7VqVbvjjjvMtzMUT5gwwRYvXuxU5+WEVkOtVKlSutt0XbdnRb9cfUCixQs3msTvxAmzXCzWCgAJTcswHzwYm9dWpUm+fNk/rmDBgs6q2goKDzzwgBMURMFGq2Ar1CgYtGrVygkfWqBx8uTJdv3111u9evXsvPPOy1EQuPzyy53z0/z5852FHjPrT1qqVClnP3SyV0AZOHCgc9tf//pXu+aaa5wuFwoQWmJBtGhkRgcOHHD+yW/Xrp1zLt2xY4fdcsstTtAIDnAzZ850goe2a9eudZ5fAUWvmRNqOXniiSecLiQtWrSwcePG2e9//3tbtWqVszr4M888Yx999JG98847TojZvHmzU+T999+3MWPGOOd8rSCuc7ZCW0QFYmTTpk2BihUrBpYtW5Z6W6dOnQJ33nlnlj9TqFChwJtvvpnutueee855nqwcPnw4sHfv3tSyefNmrYLuXI6E48cDgfz59SceCGzZEpGXAICYO3ToUOC7775ztp79+93vvlgUvXZOrV692jkPzJw5M/W2Cy+8MPDHP/4xy5/p1atX4J577snyfFWrVq3AmDFjnMufffZZoGDBgoFffvkl9f4pU6Y4rzlx4sQsX2P06NGBVq1apV4fPnx4oHnz5qc8Lvh5XnrppUC5cuUC+4N+AZMnTw7kz58/sG3bNud6//79nf07rhPU/1x11VWBa665Jst9yfjaVatWDTzyyCPpHtOmTZvAbbfd5ly+/fbbA126dAmcPHnylOd64oknAmeffXbg6NGjgdP5XHl03s7p+TtmzVKLFi1yEqb6zChJq8yePdtJf7qsBJ1ZO+P2DOsa6Lpuz4qqwJS8g0skqabGq7355ZeIvhQA4DRoxG379u2d2gdRTYY6E6tJSnT+UZ9ONUeVL1/eSpYsaZ999plt2rQpR8+/evVqZx0o1ch4VLOS0dtvv+0sgKlzmF7jwQcfzPFrBL9W8+bNrUSJEqm3dejQwak9WrNmTeptqjEpENSUoFocnYNzQt05tmzZ4jxvMF3X63tNX0uXLrUGDRo4TU5avdxz1VVX2aFDh5ymN9UUTZw40Y4fP26RFLNwc/HFFzvVcPpleEUdudS5WJeD34TgD8f06dPT3aZ2zMw+NLFUvbq7zaS/GgD4lpqG1NUjFiW3fXkVZNRcsm/fPqcjsZqcOnXq5Nw3evRopxlGzVJqxtE5SU0/6lcSLuorqvNdz5497ZNPPnFGAKuZLJyvEaxQhpWc1RynABQuqqhYv369EwoVZK6++mq78sornfsU9BS0nn/+eadf0G233eb0R8pNn5+E6XOjdsVzzjkn3W1Knuro5N2udlF16PI6Jd15553Oh0/tfuqErPa7hQsX2ksvvWTxFm7mzyfcAEgu6r4SVIEQ13Ty1TnlzTfftNdff90GDx6c2v9GK2/37t3b/vjHPzrXFQJ++OEHp2NwTmg0r/qbaMi2akhk3rx5p0xtok63CjSejRs3pntM4cKFM23FyPha6lujvjde7c0333xj+fPnd2pRwkEtHqqF0vN6AdB7neA+SHqc+vKoKNhodPOuXbuc2i+FGg3DV1FnZtWeqYJDoch3HYqzo+o5vUEeVSPqg6iqu/vvv9/pxKQe4xlDUqxVq+ZuCTcAEJ/UDKST8LBhw5xmFzWreHRuee+995wAoilKnnzySacLRE7DjUb1ahRU//79nVogPX9wiPFeQ+c4/ZPepk0bp9OymmsyjsBSbYhqjjRKSZUCGYeAq/Zn+PDhzmtpRNLOnTvt9ttvdzpAZxyAkxeagkWvoxoudURWbZf264033nDu1+9IQU6djXXeVgdtNbeVLVvWCV8Kaeeff74zMmz8+PFO2FG4i5S4CjezZs0Ked1ru1OJZ16zFH1uACB+qWnq5ZdfdpqGgvvH6B/odevWOU1ROhlraLWGUmvUU07o5K6goudXzYZCivqTBk8eqJFGf/7zn51RTZqyRK0RGgqugOK54oor7IMPPrCLLrrI9uzZ4wSK4BAm2j/1B1ItlEKSrl9xxRVO2Agn9aPR8d9zzz1OXx0FPY2OUkgTBa/HH3/cGQKvbiXaF60goN+FAo5WIbj77rudkKO+TB9//LHTUhMp+dSr2JKIErSG0+lNilTnYk2HcN11mp5bw+8i8hIAEFOHDx92ahXq1KnjzGsCRPpzlZvzd8wn8fMjOhQDABA7hJsIh5vkqhcDACD2CDcR4DXdHj5stmtXrPcGAIDkQriJAHVmP/NM9zJNUwAARBfhJkIYMQUgGSTZmBQkyOeJcBMhdCoG4GfejLcHY7VSJnzp6P9maM5slYKEnefGTwg3APxMJx/NX+KtT6T5VbwZfoHToZmgNQmhPktaYzIvCDcRwizFAPzOW7Q4pwswAtnRpH81a9bMc1Am3EQINTcA/E4nIE25X7FixYgugojkUbhw4XTLLp0uwk2EEG4AJFMTVV77SADhRIfiCGG0FAAAsUG4iXCfm5QUtwAAgOgg3ERIyZJmZcq4l6m9AQAgegg3EUS/GwAAoo9wE0GEGwAAoo9wE0GEGwAAoo9wE0GEGwAAoo9wE0E1arjbTZtivScAACQPwk0E1arlbjdujPWeAACQPAg3UQo3YVrFHQAAZINwE4VmqYMHzX77LdZ7AwBAciDcRFDRomaVKrmX6XcDAEB0EG4ijH43AABEF+Emwgg3AABEF+EmwmrWdLc0SwEAEB2Emwij5gYAgOgi3EQY4QYAgOgi3EQp3NAsBQBAdBBuotTnZudOd74bAAAQWYSbCCtb1qxUKfcytTcAAEQe4SbC8uWjaQoAgGgi3ESxaYpOxQAARB7hJgoYMQUAQPQQbqKAcAMAQPQQbqKAWYoBAIgewk0U1K7tbjdsiPWeAADgf4SbKKhTx93+/LPZ0aOx3hsAAPyNcBMFlSqZFStmdvIkTVMAAEQa4SZKc93UreteXrcu1nsDAIC/EW6ihHADAEB0EG6ihHADAEB0EG6ihHADAEB0EG6ihHADAEB0EG6iHG5++sksEIj13gAA4F8xDTdjx461Zs2aWenSpZ3Srl07mzJlSpaPf/XVVy1fvnzpStGiRS2RJvJLSTHbvTvWewMAgH8VjOWLV69e3UaNGmVnnXWWBQIBe+2116x37962ZMkSa9KkSaY/oxC0Zs2a1OsKOImgeHGzKlXMtm51m6bKl4/1HgEA4E8xDTeXXXZZuuuPPPKIU5szb968LMONwkzlypUt7vzwg9mLL5qVKGE2YkSWTVMKN+vXm7VuHfU9BAAgKcRNn5sTJ07YhAkT7MCBA07zVFb2799vtWrVsho1aji1PKtWrQr5vEeOHLGUlJR0JSJ+/dXsySfNXn8922UY6FQMAICPw82KFSusZMmSVqRIERs0aJBNnDjRGjdunOljGzRoYOPGjbMPP/zQxo8fbydPnrT27dvbz1q0KQsjR460MmXKpBaFoogu/a19OXEi04cwYgoAgMjLF1Bnlxg6evSobdq0yfbu3Wvvvfee/ec//7HZs2dnGXCCHTt2zBo1amR9+/a1f/7zn1nW3Kh4VHOjgKPXU/+dsFGgKVLE3SrgVKt2ykNee81swACzrl3Npk0L30sDAOB3KSkpTiVFTs7fMe1zI4ULF7b69es7l1u1amULFiywp59+2l5U/5VsFCpUyFq0aGFr167N8jGqEVKJuAIF3ECjlTFVMgk31NwAAJAEzVIZqakpuKYlu346ataqomFI8cBrmtq8OdO7vXCzcaPZ8eNR3C8AAJJITGtuhg0bZj169LCaNWvavn377M0337RZs2bZZ5995tx/ww03WLVq1Zx+MzJixAhr27atU9OzZ88eGz16tG3cuNFuueUWiwtefx7V3GRCGUzT8hw+7AacevWiu3sAACSDmIabHTt2OAFm69atTjuaJvRTsLnkkkuc+9UXJ3/+tMql3bt328CBA23btm1Wrlw5pxlrzpw5OeqfEw81NzoUBRoN8PrxR8INAAC+Czcvv/xyyPtVixNszJgxTolb2dTcyNlnp4Wb7t2jt2sAACSLuOtzk9Cyqbnxwo035x8AAAg/wk0Mam6EcAMAQGQQbiJRc7Nzp9mhQ5k+5Kyz3C3hBgCAyCDchFO5cu4KmZLFrMlezY1GS+VwxDsAAMgFwk04aYVyr/Ymi6apihW1srmZ5oX+6afo7h4AAMmAcBPlTsXKP/S7AQAgcgg3MehUTL8bAAAih3ATw+HgmusGAACEF+Em3BgODgBATBFuwi2bDsVCuAEAIHIIN5GquVGzlIZEhehzs22bWUpKFPcNAIAkQLiJVLg5cEArfWb6kDJl3CHhsnZtFPcNAIAkQLgJt2LFzCpUyHGn4jVrorRfAAAkCcJNjPrdNGzoblevjtI+AQCQJAg3kVCrVtoaC1lo3NjdfvddlPYJAIAkQbiJhNq13e369Vk+hHADAEBkEG4ioU6dHIcbTeR39GiU9gsAgCRAuIlkuNmwIcuHVK9uVqqU2fHjjJgCACCcCDcxapbSApo0TQEAEH6Em0iGmz173JIFwg0AAOFHuImEkiXNzjwz26Ypwg0AAOFHuIl07Q3hBgCAqCLcxMGIKc1SrI7FAAAg7wg3MexUrImMixd3h4KvWxe9XQMAwM8INzEcDp4/v1mjRu7lVauitF8AAPgc4SaGzVLSpIm7XbkyCvsEAEASINxEo0NxIJDlw5o1c7fLlkVpvwAA8DnCTaQXz9y/3+y337J8WPPm7nb58ijtFwAAPke4iZSiRc2qVs22acqrudESDAcORGnfAADwMcJNjOe6qVjRrHJlt+VqxYro7RoAAH5FuImDTsU0TQEAED6EmxjX3AidigEACB/CTRzV3BBuAADIO8JNNMJNNtMPezU3apYKMWocAADkAOEmkurXT6u5OXEiy4c1bGhWuLDZvn3ZtmABAIBsEG4iqXp1syJFzI4dM9u8OcuHFSqUtogmTVMAAOQN4SaStHhU3bppE9nkoN/NkiVR2C8AAHyMcBOtpqlswk3r1u520aIo7BMAAD5GuImzcLNwIZ2KAQDIC8JNtMLNjz9m2yxVoIDZ9u1mv/wSnV0DAMCPCDeRdtZZOaq5KVbM7Jxz0mpvAADA6SHcRKvm5qefzE6eDPnQVq3cLeEGAIDTR7iJtBo13LHeR45k294U3O8GAACcHsJNpBUsmDZTMZ2KAQCIOMJNHI2Y0jIMquT57TezjRujs2sAAPhNTMPN2LFjrVmzZla6dGmntGvXzqZMmRLyZ959911r2LChFS1a1Jo2bWqffvqp+SXcaDLjpk3dyzRNAQCQgOGmevXqNmrUKFu0aJEtXLjQunTpYr1797ZVq1Zl+vg5c+ZY37597eabb7YlS5ZYnz59nLJy5UrzQ7iRNm3c7bx5Ed4nAAB8Kl8gEF+9O8qXL2+jR492AkxG11xzjR04cMA++eST1Nvatm1r5557rr3wwgs5ev6UlBQrU6aM7d2716ktigrVRvXs6bY7ZbN41GuvmQ0YYNa+vdk330Rn9wAAiHe5OX/HTZ+bEydO2IQJE5zwouapzMydO9e6du2a7rZu3bo5t2flyJEjzi8kuMS05iabLKlQ4zVLaYAVAADInZiHmxUrVljJkiWtSJEiNmjQIJs4caI19pbIzmDbtm1WqVKldLfpum7PysiRI52k55UaGpodbbVru9MPHzxotmVLtjmoQgWzo0fNFi+O2h4CAOAbMQ83DRo0sKVLl9r8+fNt8ODB1r9/f/vuu+/C9vzDhg1zqrC8snnzZos6DYGqV8+9/P33IR+aL19a7c2cOVHYNwAAfCbm4aZw4cJWv359a9WqlVPL0rx5c3v66aczfWzlypVtuxZfCqLruj0rqhHyRmN5JSYaNsxRuBHCDQAACRxuMjp58qTTTyYz6oszffr0dLdNmzYtyz46caVRo9MKN/HV3RsAgPhXMJYvriajHj16WM2aNW3fvn325ptv2qxZs+yzzz5z7r/hhhusWrVqTo2O3HnnndapUyd74oknrFevXk4HZA0hf+mllyzu5aLmRjMVa2JjdSXasCFtgmMAABDnNTc7duxwAoz63Vx88cW2YMECJ9hccsklzv2bNm2yrVu3pj6+ffv2TgBSmFHz1XvvvWeTJk2yc7zltBMh3Kxene1DtUJ4y5buZZqmAABI8HluIi0m89zI7t2axMfbCbNSpUI+/M9/NnvqKbPBg82efz46uwgAQLxKyHlufK9cOY1bdy+vWZPtwzt0cLdffRXh/QIAwGcIN3Ha76ZTJ3erlSV27ozwfgEA4COEmzgdMXXmmWZeV6LZsyO8XwAA+AjhJk5rbuSii9ztzJkR3CcAAHyGcBOnI6aEcAMAQO4RbmIRbn780ez48Wwf3rGjuxyDslCI5bMAAEAQwk00adFOTWJz7JjZ+vXZPvyMM8yaNXMv0+8GAICcIdxEU/78WinUvUzTFAAAEUG4ibYmTdLGeOcA4QYAgNwh3ERb06budsWKHD1c/W4KFDD74QezjRsju2sAAPgB4SbOw03ZsmZt27qX/7eeKAAACIFwE23ezHxaguHo0Rz9SPfu7nbq1AjuFwAAPkG4icWIqTJl3KHgOVhjSnr0cLdffOEOtAIAAFkj3ESbJq7xam9y2DTVooW7HMO+fWZz5kR29wAASHSEmwTod6MR5N26uZdpmgIAIDTCTQKEG6HfDQAAOUO4SZBwc+mlbovW0qVmW7dGbtcAAEh0hJtY8PrcbNpktndvjn5EfW5at3YvT54cwX0DACDBEW5ioVw5s+rVczVTsfTu7W4nTYrQfgEA4ANhDTfr1q2zS9V+gux5K2IuW5bjH+nTJ21I+P79EdovAAASXFjDzb59+2z69OnhfEr/0vhuWbIkxz/SuLFZ/fpmR44wWzEAAFmhWSpWWrZ0t4sX5/hH1KGYpikAAEIj3MQ63GjEVA6XYQhumvrkE2YrBgAgM4SbWKlVy+1YrISSi07F7dq5I6f27DH76quI7iEAAAmpYG4e3KJFC8untpEsHDx4MBz7lBz0e1TtjfooqWnKq8nJRoECZpddZjZunNnEiWZdukR8TwEA8G+46eO1iSA8gsNNLvzhD264ef99s6eecgMPAAA4jXAzfPjw3DwcEehULBptX7asO1Px11+bdeoUmd0DAMCSvc/N8uXLrXDhwuF8yuQIN5rr5vjxHP+YfsWXX+5enjAhQvsGAECCCmu4CQQCdjwXJ+mkp0lrSpY0O3zY7Pvvc/Wj117rbt97j1FTAABEdLRUqA7HyCB//rTJ/BYuzNWPXnSRO2rq11/NZsyIzO4BAJCIGAoea23auNv583P1YwULml15pXv57bcjsF8AACRDuElJSQlZtPwCckkT18i8ebn+Ua9p6oMP3CUZAABALkdLlS1bNmSzk/rc0CyVS23butvly80OHDArUSLHP3rBBWbVqpn98ovZ5MlpnYwBAEhmuQo3M2bMILyEW/XqaQlF/W5yMa5bXXauv95s1CizV18l3AAAkOtw07lzZ35rkaq90Yx8aprK5aQ1/fu74ebTT822bzerVCliewkAgP/63OTPn98KFCgQshRUT1dErd9Nw4ZuNjpxwmz8+PDvGgAAiSZXSWSiFjPKwty5c+2ZZ56xkydPhmO/kovX72buXHVcctedyoUBA9xcpKapu+/O9Y8DAOAr+QLqBZwHa9assb/97W/28ccfW79+/WzEiBFWSytexymN6ipTpozt3bvXSpcubXHh0CEz7YsmQFy/3qx27Vz9uFYIr1zZHTGlbjutWkVsTwEAiPvz92nPc7NlyxYbOHCgNW3a1JmVeOnSpfbaa6/FdbCJW8WKmZ17blrtTS5pnSktpimqvQEAIJnlOtwoMd13331Wv359W7VqlU2fPt2ptTnnnHMis4fJon17d/vVV6f142qakjfecCuCAABIVrkKN48//rjVrVvXPvnkE3vrrbdszpw5duGFF0Zu75KJN0pq9uzT+vGuXc1UabZ7NzMWAwCSW6763Gi0VLFixaxr167OyKisfKApc+NUXPa5ES0SpcWiZMeOtMu5MHKk2f33m513Xq5XcwAAIK7l5vydq9FSN9xwA5P4RUqFCmZNmpitWmX25ZdmV1yR66e4+Waz4cPNvv3WbPFis5YtI7KnAADEtVyFm1fprRr5pimFGzVNnUa4qVjR/bEJE8zGjjX7978jspcAAMQ1VgX3Ub8bGTw4rWOxhogDAJBsYhpuRo4caW3atLFSpUpZxYoVrU+fPs68OdnVHqlpLLgULVrUfBVuVqww27XrtJ5C/bvVuqURU6+9Ft7dAwAgEcQ03MyePduGDBli8+bNs2nTptmxY8fs0ksvtQNaHTsEdSTaunVratm4caP5ghaG0noK6uN9mkPC1SXqttvcy8884y7LAABAMolpuJk6daoNGDDAmjRpYs2bN3dqZTZt2mSLFi0K+XOqralcuXJqqeSn1SK9xUmnTz/tp9BimuXKma1bZ/bhh+HbNQAAEkFc9bnR8C4pX758yMft37/fmQm5Ro0a1rt3b2cywawcOXLEGT4WXOLapZe6288/P+2nKFHCbNAg9/ITT4RpvwAASBBxE2604OZdd91lHTp0CDnbcYMGDWzcuHH24Ycf2vjx452fa9++vf38889Z9uvRuHivKBDFtS5dzDSHkPoe5aG5behQs0KFzObMOa3FxgEASN6FM8Nl8ODBNmXKFPv666+tevXqOf459dNp1KiR9e3b1/75z39mWnOj4lHNjQJO3E3iF+yCC8y++cbsxRfNbr31tJ9GSzKoU/FVV5m9805Y9xAAAP8tnBlOQ4cOdZZ0mDlzZq6CjRQqVMhatGhha9euzfT+IkWKOL+E4BL3wtA0JXff7W7ff99dbBwAgGQQ03CjSiMFm4kTJ9qMGTOsTp06uX6OEydO2IoVK6xKlSrmG926udsvvjA7fvy0n6ZZM7NLLlGTH31vAADJI6bhRsPA1W/mzTffdOa62bZtm1MOBS1rrSUfhg0blnp9xIgR9vnnn9u6dets8eLF9sc//tEZCn7LLbeYb7Ru7Q53UgfrBQvy9FR/+5u7/c9/zLZuDc/uAQAQz2IabsaOHeu0nXXu3NmpefHK20HLWmtouOay8ezevdsGDhzo9LPp2bOn0wan1ckbN25svqEOxVrmW6ZMydNTXXSRWYcO6ntkNnp0eHYPAIB4Fjcdii3ZVwXP6PXX3Qlr1La0bFmenuqzz8y6dzcrVsxswwZ3DSoAABJJwnUoRiZ69XJrcJYvz3NvYPVPPu88d0kG+t4AAPyOcBOvzjjDHRIuH32Up6fSkgz/+Id7+bnnzH79NQz7BwBAnCLcxLPevd1tGNZQ6NnTrGVLMy3bRd8bAICfEW4SIdx8+eVprxIeXHvz8MNpC2pmMaEzAAAJj3ATz+rWNdNSFFra+9NPw9KNRy1dhw+nBR0AAPyGcJMotTeTJuX5qVR789hj7uVx48y+/z7PTwkAQNwh3MS7yy93t5Mnm+3bl+ena9/ezUuatfj++/O+ewAAxBvCTbxr0cLsrLPctqQ8jpryPPqoWf78ZhMnms2dG5anBAAgbhBu4p3akvr2dS9PmBCWp9Rkzjfe6F6+4w63FgcAAL8g3CSCa65Jm2o4j6OmPI88YqYJHhcuNHv11bA8JQAAcYFwkwhU1aJlGI4dc9uSwqBSJbPhw9MW19yzJyxPCwBAzBFuEsW117rbt94K21MOHWrWsKHZzp0MDQcA+AfhJtHCzcyZYZuBr3Bhs6efdi8/+6zZypVheVoAAGKKcJMo6tQx69jR7f2rFcPDRItq9unjzhM4cKC7BQAgkRFuEok3xEkz8AUCYXta1dqUKmU2b57Z2LFhe1oAAGKCcJNIrrzSrGRJs59+Mvvqq7A9bfXqZqNGuZeHDTPbtClsTw0AQNQRbhKJgo03LPyVV8L61IMGubMX799vNnhwWCuGAACIKsJNornpJnf7zjthWY7BoxmL//1vt5Ox1uh87bWwPTUAAFFFuEk07dqZNWhgdvBg2GYsDp5Ox5v7RjMXr18f1qcHACAqCDeJuBzDLbe4l59/PuztR/fdZ9ahg1spdP31jJ4CACQewk2iNk0VLWq2dKnZnDlhfeoCBcz++1939NQ335g99lhYnx4AgIgj3CSi8uXN+vVzL//rXxGZUsd7WjVTsXI4ACCREG4S1ZAh7va998y2bg3706tJSgOzjh83u/pqd4kGAAASAeEmUbVo4Y7dVvrQMKcIdO3R06rvslZ7UEUR/W8AAImAcJPItPKlvPCC2ZEjYX969bt5/32z4sXNpk0zGzEi7C8BAEDYEW4S2RVXmFWr5jZLvflmRF6iSROzl15yLyvcfPBBRF4GAICwIdwkMs24d9dd7uXRo91FNSNATVKa90b++EezRYsi8jIAAIQF4SbR3XqrWenSZqtXm02eHLGXeeIJs+7dzQ4dMvv9781++SViLwUAQJ4QbhKdgo0WhvJqbyKkYEF3QmQ1U23ZYva735mlpETs5QAAOG2EGz+4806zQoXclcIjOClNmTJmH39sduaZ7vyBqsFRTQ4AAPGEcOMHVau6E9PIyJERfSlN8Dd1qjuSavZsdw6cY8ci+pIAAOQK4cYv/vpXd2lvVa1EuMdvy5Zmn3zirgCh7Y03MgcOACB+EG78QrPtXXede/nhhyP+ch07upMjqy/OG2+YDRjgzicIAECsEW785O9/T6u9Wbgw4i/Xq5c7vY4CzvjxbraiiQoAEGuEGz85++y0BTWjUHsjV13l1uBoyp133zW78kqzw4ej8tIAAGSKcOM3Dz7o1t6oM8yCBVF5yd69zT780O2D89FHZpdcYvbbb1F5aQAATkG48WPtjaYRlgceiNrLaoK/KVPc4eJff+2u6bluXdReHgCAVIQbPxo+3J33Rqtdfv551F62c2ezb74xq1nT7IcfzNq2da8DABBNhBs/qlvXbMiQtCHiURynrRmM581zh4vv3OkGnmeeMQsEorYLAIAkR7jxc98btREtW+aO1Y6iKlXcCf6uucYdHq4JlNXP+cCBqO4GACBJEW786owzzIYNSws6UV4noWRJs7feMhszxqxAAfeyanO+/TaquwEASEKEGz+74w6zGjXMNm9224aiLF8+s7vuMps5010hQv1w1NF4xAgm/AMARA7hxs+KFTP7f//PvaytlvOOgQsvNFuxwm2mUvcf9XdWyNHimwAAhBvhxu80LPz8883273c7F8dI+fJmEya4MxqrK5Cm4GnVyuzuu8327YvZbgEAfIhw43ea0O9f/3LbiNSx+MsvY7o7ffuaffedW4tz8qTbJ6dRI7PXX2fxTQCAD8LNyJEjrU2bNlaqVCmrWLGi9enTx9asWZPtz7377rvWsGFDK1q0qDVt2tQ+/fTTqOxvwmrd2mzgQPfy0KEx7/Ci/jeqxZk61axePbNffjHr39/tcKyJABk2DgBI2HAze/ZsGzJkiM2bN8+mTZtmx44ds0svvdQOhBgzPGfOHOvbt6/dfPPNtmTJEicQqaxcuTKq+55wHn3UbRtS55fnn7d40K2buzuPPeY2VS1fbtazp9lFF7lzDxJyAACnI18gED+nkJ07dzo1OAo9HTt2zPQx11xzjRN+PtHaSf/Ttm1bO/fcc+2FF17I9jVSUlKsTJkytnfvXitdurQllRdfNBs0yKxUKbdtqHp1ixe7dqkmz+zZZ82OHHFvU03O3/5mdvnl7nByAEDySsnF+Tuu+txoh6W8ahiyMHfuXOvatWu627p16+bcnpkjR444v5DgkrRuucXtXKwevIMHx1XViN7y0aPNfvzRnfSveHGzxYvNrr7anXD5kUfMtm2L9V4CABJB3ISbkydP2l133WUdOnSwc845J8vHbdu2zSpVqpTuNl3X7Vn161HS80oNzfuSrFT98fLL7rpTqvl6+22LN3p7nnrKbONGs3/8ww09mza58xDqPoUd7frRo7HeUwBAvIqbcKO+N+o3M0E9TcNo2LBhTo2QVzZrQrtkpsWflBTk9tvNfv3V4lGFCmYPP2z288/uSKp27dx+0O++a3bZZe4SD7fe6k4QyISAAIC4CzdDhw51+tDMnDnTqmfTD6Ry5cq2ffv2dLfpum7PTJEiRZy2ueCS9NSRRbVjCjaaQjjO5yG8/np1JHcn/dOky3qr1Ufn3/8269LFrGJFs+uuc0e6//ZbrPcYAJDU4UZ9mRVsJk6caDNmzLA6depk+zPt2rWz6dOnp7tNI610O3KocGG3eUpz4CgRvP9+rPcoR5o3N3v6abc254sv3C5Earbavdtdu0rzFSronHee2V/+YvbRR24IAgAkl5iOlrrtttvszTfftA8//NAaNGiQerv6xhTTv+xmdsMNN1i1atWcvjPeUPBOnTrZqFGjrFevXk4z1qOPPmqLFy8O2VfHk9SjpTK6/353iJI3RFwT0CQYNUnNn+/2w5k82T2MjPSxUODR6CuVZs3MSpSIxd4CAE5Xbs7fMQ03+TRrbiZeeeUVGzBggHO5c+fOVrt2bXv11VfTTeL34IMP2oYNG+yss86yxx9/3HpqgpQcINwEUa9c1XhpWNIll7iz6qk2J4GpVmf2bLOvvnInY169+tTH6BCVpRs3drcNG7pbFc23AwCIPwkTbmKBcJPB99+71RmHDrltPurU4iM7drj9dZTfVBYtCj2kXM1atWqZ1ayZvmiklvr6nHmmWdGi0TwCAIAQbkIg3GRCMxYPGaLe124bjzq3+NjWrW7nZOU6rfbhbXM6j47mQFQI8ooCT7lybq2Pij5W3mXvun5GLa0KRhqJn+jUHHj4cFpRNg6+Huo+TdLobYNLTm8L/sbK7NtLXcr0e1bxfucqeg/UAnvGGem31aqlhVcmiwTiF+EmBMJNJvQR0PhqdVqpX99s4cKkbJ/RHJI//WSm2QI0t05w0W2qBTp2LO+voxOoTrreide7rKITc8GCpxb9TGa3eX+92oa67F3XYqUKJjoOr4S67l1WC2ZwQPHjIqf6nSroqKburLPcBV3VdKmtavMSvMUWSHiEmxAIN1nQGGo1T+lMrvUO3nvPXUkcqfSXogCkkKOyc2fa5T173Ps0Aba2GS+HWC4t4QXXlGRWYxJcVDmYcRtcsrtNr5WxdiX4Y6r3KDiIebVG2mpibn3MNYLO22o2BC3cqhIqsGnG7BYtzNq0SSv6P4A/ESB6CDchEG5C+PZbswsucP9Vf+IJs7vvjvUe+Yb+ytSkopNsxuKdfFV0YlZtiU602gaXzG7zTq7aZnU5+DYVNYuplkLb4JLdbRkDi64rcPihRkO/SzVLKttrdmw1U6ozusoPP6StdxZMTVqdOrkLvapofkzCDhA5hJsQCDc57H+jf49nzXLDDpDEFCq15tmCBWllyZJTA4/6XmlSSQ3c7NHDvQ4gfAg3IRBusqGPQ79+7qx4+nZWbU7t2rHeKyCuqIZNI++0/If+B/j6a7fmzaMaHK1R26uXW849l1odIK8INyEQbnJAHUQuvND991Qz4H3zjTvkB0CWYUf/B2iqKE0ouWxZ+vvVUVl99lVUu8N0AkDuEW5CINzkYjY89ZpURwT96/nhh4yTBXLx5/Ppp+4ARC0VcvBg+s7Jl17qBh39aVWqFMs9BRIH4SYEwk0uqHNBx45uj9c//9nsySdjvUdAwtGfz4wZZh9/7BaNzMrYfPX737thh07JQNYINyEQbnLpnXfMrrnGvTx6tLsiJYDTom9btfYq5GhhV82aHUxrB3vNV/q/QkPfAbgINyEQbk7D44+b3Xefe/mVV8z+t+4XgLxRLY766CjoTJ+efgSWvp66dzfr1s3tAse8Okh2KYSbrBFuTtO995r93/+5/W4++MCtRwcQ1n7806a5tToKPJocMpiWh1DIUWnb1qxpUzomI7mkEG6yRrg5TfqY3HSTmVZn18xt6il58cWx3ivAl7RMhkZfKeRolXtd1oisYJpgUX10WrVyJxfXtlkzt8MyEA+f4XBP8Em4CYFwk8dpXK+4wq1D17+M+uYl4ABR6ZSsgPPVV27R8m9aQiIjnUy0FpYXdrTVHDtaNBQIN6WH7dvdxYczFn32VMkfToSbEAg3eaROAQo4qrlRwFEdeteusd4rIKnoW1uLuWoiQXVK1lYlY1OWqJ+OFgLVyUYjs/T/iKavov8Ockor8mhR4cxCjNbOy0zDhu7yJeFEuAmBcBOmgHPllW7NjQLOpElur0cAMaNv8q1b3bATXBSCMqpY0Q056jr3u9+ZlSwZiz1GvNmzJ/MAo2CjivvMqLZQo/wUZlRrqK1KgwZmFSqEd/8INyEQbsIYcK66yq250aqKr79udu21sd4rABlo9XoNP1dT1pdfus1awZMK6v8TrYV1/fVu0NGfM/zdF2bz5sxDjOZszUqJEmnBJbhoFF+0OrYTbkIg3ISRejjqG1Fz4chTT5ndeWes9wpANv+XzJvnLhXx3ntma9em3VelitnNN5sNHGhWs2Ys9xJ5lZLirmivFe6Di24LXgctIy0VklmI0e2xbsok3IRAuInAvwF33WX27LPudc2HM3Jk7P8KAGRL3/5aB2vCBHcKK6/PjmZ86NvX/XNW/xzEJzUVbdhwaoBZsyZ0LYxq584++9QAo9vi+bRIuAmBcBMB+giNGmV2//3udTVX6ZtS9ZgAEqYiVt3nXnjBXe3co9mS9aetuXUQGxoZl1mAUa2bOvtmpVIlt+9LxqI+MppKINEQbkIg3ESQ5sC59Vb3r03jT7XYJnXbQMLRyCv9v/L+++7/LqIOyA8+aNapExWzkaCmInXc/fHH9E1I2mY27N+j/i4aDZcxwJx9tlnZsuYrhJsQCDcR9vXXZpdf7vZiPPNMd6KDCy6I9V4BOA06sWr1FY0X8EbL6M9ZIUcrmxNyckcdub0Ao1oXFe+yVpIPpUaNU8OLtvr/MdyT5cUrwk0IhJso2LTJrHdvs6VL3cb7Rx91F9xMlr9AwGc2bnRDzssvp61/1bq1G3LUbMWfdvplNIIDTHCQCV4RPjNlyrijjzLWwqhmhlZ+I9yEQriJ4l+4hly89ZZ7XSsA6t8/1eYASEhbtpg98YTbL8cbTq41rh54wJ36Sv/L+J1a3VXLsn59WlGnXu+y5hoKRU1FCisqCjIq3uUzzqA2LBTCTQiEmyjSR0v/6t1+uzt/fNWqZq+9xozGQIJTq7NmftAgyX373NvUTDJ4sFm/fon9P4ya3xRQVFuVMbioKNicOBH6ORRSMgYX73L58tE6Ev8h3IRAuImBFSvMrr7anSVKBg1y67hZ8AZIaLt3uwFHQUeXvWHGmgxQf/KauLxcOYu74KJJ7BRSVLzL3lb3a4aLULR2cO3a7qgjb+uVevXi65j9hHATAuEmhs1Uf/2r2fPPu9f1jTBunNlFF8V6zwDkkWpv/vtfdwYIzYTsUTNVhw5m7du7Q8lbtDCrXj28fXQUWLRsgBZwVNH8LqEuZxdcRMOktZ9eYMkYYCpXpp9RLBBuQiDcxNiMGWY33eTW+cqNN5o99lhi12MDSFdRO368u/Tcd99lXuuh2g2FBzXfqJlG/VAUKFQUiBQc1HFZrdle0VBphRjVEHlF171msZzSa2i2XY0+0j542+DLmh+G8BJ/CDchEG7igL6N7r3X7MUX3ev6ZtOIKs2Rkww9EoEksW6d2fTpZvPnu0Ut01ktwJhXCkkKJSqqWfEuB19Xtz+CS+Ii3IRAuIkjc+aYDRniDhmXli3Nxowx69gx1nsGIAIUbDRThIZHq5lo1y637N3r3qeizroqmpyuWDF36xX9H6SiPi0q3mWv5gf+lkK4yRrhJs7o20zjSjVhhr7hRL0RtT4Vi9oAAE7j/E3lHGJL/24NHepOhapRVGqWUmN9s2ZufxyNvQQAIBcIN4gPaggfO9btgajZwFShqLWqNDHEDTeYrV4d6z0EACQIwg3ii2YCe/dds3nz3Eky1PiuMaZNmrihZ8GCWO8hACDOEW4Qn84/32zqVDfM9Onj1uRoieLzzjNr185d1uHo0VjvJQAgDhFuEN+0Ot/Eie7kGddf705/qlqd664zq1XL7OGH3alFAQD4H8INEoNGTmnhTY0jVaDRxBUaS/rQQ27IufRStzZHM30BAJIaQ8GRmNQkpWaql14ymzUr7fYyZcyuvdbsmmvc+XKYFBAAfIF5bkIg3Ph0GlStNq7iLesgFSuaXX65u4IfQQcAEhrhJgTCjY9pRbyZM93mKfXT0dSnHq1d1auXWy65xK3hAQAkDMJNCISbJHHsmLtIp4aVZww6mjjwggvMevY069HDHWaeL18s9xYAkA3CTQiEmyQNOl9+afbpp2aTJ7uzIQdT81XnzmYXXWTWpYs7cSBhBwDiCuEmBMIN7Kef0oKOQk/GEVZaOlhBp317d06dpk1ZlQ8AYoxwEwLhBukcOWI2f77bV0dl7txTJwcsXtysTRs36LRt6xYtFwEAiBrCTQiEG4SkWhwFnNmz3ckCFXy81cqDVatm1qJFWmnZ0qxmTZqzACDZw82XX35po0ePtkWLFtnWrVtt4sSJ1kdT7Wdh1qxZdpGaCzLQz1bWpG45QLhBrkdgadFOBR2FHhUt7pmZcuXSwo4mHVRH5UaNzEqWjPZeA4Dv5Ob8HdOOBAcOHLDmzZvbTTfdZJdrPpIcWrNmTboDq6gOoUAk5M/vhhSVm292b9u3z2zZMrMlS8wWL3a3q1aZ7d7tjtBSCVa7tlnjxmnP44WeEiVickgA4HcxDTc9evRwSm4pzJQtWzYi+wRkq1Qpdyi5SnDfHQUcBZ2lS93LquHZvt1swwa3qBNzsBo13FXQNTpLxbtcp45Z4cJRPywA8IuEHAJy7rnn2pEjR+ycc86xhx56yDp06JDlY/U4leBqLSDsihRx+92oBPv1VzfkKOx4Rdd37HAX/FSZPj39z2gmZdX2BIeeevXc21SKFYvqoQFAokmocFOlShV74YUXrHXr1k5g+c9//mOdO3e2+fPnW8uMJ5X/GTlypD2shRaBWKhQwV36QSVj6PnhB7Mff0zbeuXAAXe4usrUqac+p/qXqXYnuCj0aKvaIK2cDgBJLG5GS+XLly/bDsWZ6dSpk9WsWdP++9//5rjmpkaNGnQoRnzSn+PWrWlBxws+Wj9r/Xq3v08oqvWpXj0t8GgEl4pCj7elrw+ABJQwHYrD4bzzzrOvv/46y/uLFCniFCAhaCi5JhFU6dTp1OCjZSQUclTUj8e77F1XkNfiocELiGZUvnz6sJNxq9em9gdAAkv4cLN06VKnuQpIiuBzxhluad0682Hr27alDzvq07NpU9pWNT8KSCoa8ZXVCDH9TXlhJ7h4t2khUj0OAOJQTMPN/v37be3atanX169f74SV8uXLO01Nw4YNs19++cVef/115/6nnnrK6tSpY02aNLHDhw87fW5mzJhhn3/+eQyPAogTChterU9Wnew1IWHGwBO8VdFaXL/84hbN65MZjeZS81fG8BNcNO8PkxoCSLZws3DhwnST8t19993Otn///vbqq686k/Nt0pfu/xw9etTuueceJ/AUL17cmjVrZl988UWmE/sByESZMm7RJIOZUe2PRnJlFnq8oj5BWqJC/YBUsqJlK0KFHxUNqwcAv3YojhZmKAbyyKvZyRh6gotGg+WE5qsKFX5UO1S0aKSPCEACSKoOxQCiTJ2NvTl3Qq3R9fPPoQOQmsj27HHLihVZP5f69wQHHo0Eq1/fnfunbl3m/QFwCsINgPBT4PAmIcyKOjeHCj8qBw+a7dzpFi11kRnV7ijoKPB4oUdbvTbregFJiWYpAPFJX01arytj3x/189EEh5r/J7sZx2vVctf18oq3rhd/+0DCSZhVwWOBcAP4hL66fvvNDToadakSfFm1PVlRbY+CjlZw95bNUBMXo7uAuEW4CYFwAyQJBZ/Vq9PW9tJWZcuWzB+vUWTnnpsWds47z23aIvAAcYFwEwLhBkhyaupS6Fm+3F3FXUWXg5ZpSaUJE9u1c0v79mZt2rB8BRAjhJsQCDcAMh3ersCjTssqixa5JWPg0dpdzZu7kyR27uwukaEABCDiCDchEG4A5IgmKly61GzOnLSi+X2CqclKYUcTiXbpYnbhhW7zFoCwI9yEQLgBcNo0Wksh56uvzGbOdPvwZFwCo1UrN+ioXHCBO1MzgDwj3IRAuAEQNlqodNYsN+jMmOGO0sq4Bpf661x8sVvUZ4cV14HTQrgJgXADIGI0K7MXdKZPd2t6gmktrY4d08KO1vhidXUgRwg3IRBuAESFvlpVk6OQo6LAs2vXqUtLqPnKCzuaawdApgg3IRBuAMSEVlxftiwt7Hz5pbu8RDCt1+WFHW0rV47V3gJxh3ATAuEGQNyMxpo/Py3szJtndvx4+sdoFmUFHQ0516SC1aoxqSCSVgrhJmuEGwBxaf9+dxSWF3Y0DD0j1eQo5KhjsrYamcU8O0gSKYSbrBFuACSEX391R2J5tTorVpidOHHq46pUMWvaNK2ok7IWCdXK7ICPEG5CINwASEjqn6OlIhYsMPv2W7doodDMaARW/fpu0PGKgo9uK1gw2nsOhAXhJgTCDQDf2LfPXRRUtTrBRYuGZkbz7jRqlD70qNSqRV8exD3CTQiEGwC+pq90TS6o0LNyZfpy4EDmP1O+vFnbtu7ioN4CoSVLRnvPgZAINyEQbgAk7VD0TZvckKPaHS/wfP+9O3Ir4wKh6qzctatbFHiKFInVngMOwk0IhBsAyLAiuubfCV4gNOPMyuqcrJmVvbDTrBkzKyPqCDchEG4AIBsKN5pR+Ysv3KJmrmAVKrgroXszK9erR58dRBzhJgTCDQDkgk4R6r+jkDNtmtns2af23alZMy3odOhAB2VEBOEmBMINAOSxGUvD0BV2vDl4dFuwihXdSQa9CQc1IovZlZFHhJsQCDcAEEaqxfFmVtakg5pZOeMyEqLvW00uqHL22W7tjopqfTQRIX14kA3CTQiEGwCIoMOH3ckGVbujtbMWL3ZXR89sdmVPoUJm1aubVark1vqoaMV077L6+JQp4xZ9b2tbvDg1QacrEHBr2w4dcieH1Da3lxVqQ5XWrc0+/TRm52+mqgQAhE/Rombt2rnFo6HmP/5o9t13bv8dzay8caNbfvnFPdGuX++WnNJMy17Q0bZUKXdUl0KPtsGXM96myQwVqPQcwdusbtPQ+OAg5V3O7LbgywoRqsVSsFPJ7HKo+/V7UzlyxC3e5Yzb7O47lElA0dQAkaTlQ2KImhsAQOzoJL5lizsHz86dZjt2uMW7rK1OlHv3uiUlJfIn5mSSL1/mgTDUZZUSJUKXsmXNatQI665ScwMASAyqIVG/G5Wc0P/javbwwo5XtKp6dk0q3nXVaihUqcZIxbuc1W3BfYi8+oDgeoGsLnvHp5oflawuZ3Wfapg0eaK3Db6c09uKFAkdVvQ4HzbvEW4AAIlDJ2ItDaGiEVhAJuieDgAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfIVwAwAAfKWgJZlAIOBsU1JSYr0rAAAgh7zztnceDyXpws2+ffucbY0aNWK9KwAA4DTO42XKlAn5mHyBnEQgHzl58qRt2bLFSpUqZfny5Qt7qlRo2rx5s5UuXdr8xu/HlwzHyPElPr8fo9+PLxmOMSVCx6e4omBTtWpVy58/dK+apKu50S+kevXqEX0NvZl+/MAmy/ElwzFyfInP78fo9+NLhmMsHYHjy67GxkOHYgAA4CuEGwAA4CuEmzAqUqSIDR8+3Nn6kd+PLxmOkeNLfH4/Rr8fXzIcY5E4OL6k61AMAAD8jZobAADgK4QbAADgK4QbAADgK4QbAADgK4SbMHnuueesdu3aVrRoUTv//PPt22+/tUQwcuRIa9OmjTNjc8WKFa1Pnz62Zs2adI/p3LmzM5tzcBk0aFC6x2zatMl69eplxYsXd57n3nvvtePHj1s8eOihh07Z/4YNG6bef/jwYRsyZIidccYZVrJkSbviiits+/btCXN8+txlPD4VHVMivn9ffvmlXXbZZc4spNrXSZMmpbtfYyD+8Y9/WJUqVaxYsWLWtWtX+/HHH9M9ZteuXdavXz9nArGyZcvazTffbPv370/3mOXLl9uFF17o/M1qNtXHH3/c4uEYjx07Zvfdd581bdrUSpQo4TzmhhtucGZWz+59HzVqVFwcY3bv4YABA07Z9+7du/vmPZTM/iZVRo8eHffv4cgcnBfC9b05a9Ysa9mypTOyqn79+vbqq6+G5yA0Wgp5M2HChEDhwoUD48aNC6xatSowcODAQNmyZQPbt28PxLtu3boFXnnllcDKlSsDS5cuDfTs2TNQs2bNwP79+1Mf06lTJ+eYtm7dmlr27t2bev/x48cD55xzTqBr166BJUuWBD799NNAhQoVAsOGDQvEg+HDhweaNGmSbv937tyZev+gQYMCNWrUCEyfPj2wcOHCQNu2bQPt27dPmOPbsWNHumObNm2aRkAGZs6cmZDvn17/gQceCHzwwQfOcUycODHd/aNGjQqUKVMmMGnSpMCyZcsCv//97wN16tQJHDp0KPUx3bt3DzRv3jwwb968wFdffRWoX79+oG/fvqn36/grVaoU6Nevn/PZf+uttwLFihULvPjiizE/xj179jjvxdtvvx34/vvvA3Pnzg2cd955gVatWqV7jlq1agVGjBiR7n0N/ruN5TFm9x7279/feY+C933Xrl3pHpPI76EEH5uKzg/58uUL/PTTT3H/HnbLwXkhHN+b69atCxQvXjxw9913B7777rvAs88+GyhQoEBg6tSpeT4Gwk0Y6ItnyJAhqddPnDgRqFq1amDkyJGBRKMTpf5QZ8+enXqbTo533nlnlj+jD23+/PkD27ZtS71t7NixgdKlSweOHDkSiIdwoy/JzOhEUqhQocC7776betvq1aud34FOKolwfBnpvapXr17g5MmTCf/+ZTxp6JgqV64cGD16dLr3sEiRIs4Xv+hLUj+3YMGC1MdMmTLFObH88ssvzvXnn38+UK5cuXTHd9999wUaNGgQiLbMTowZffvtt87jNm7cmO7EOGbMmCx/Jl6OMatw07t37yx/xo/voY63S5cu6W5LlPdwR4bzQri+N//61786/3gGu+aaa5xwlVc0S+XR0aNHbdGiRU7VePD6Vbo+d+5cSzR79+51tuXLl093+xtvvGEVKlSwc845x4YNG2YHDx5MvU/HqSr0SpUqpd7WrVs3Z/G0VatWWTxQs4Wqj+vWretUdau6VPTeqRkg+P1Tk1XNmjVT379EOL7gz+P48ePtpptuSrcwbKK/f57169fbtm3b0r1fWmtGTcHB75eaMVq3bp36GD1ef5fz589PfUzHjh2tcOHC6Y5ZVe+7d++2ePy71Pup4wqmJgw1C7Ro0cJp7giu8o/3Y1RzhJoqGjRoYIMHD7bffvst9T6/vYdqrpk8ebLTtJZRIryHezOcF8L1vanHBD+H95hwnDuTbuHMcPv111/txIkT6d5A0fXvv//eEm3F9Lvuuss6dOjgnAQ91113ndWqVcsJB2r/VX8A/XF98MEHzv062WR2/N59saYTn9px9SW6detWe/jhh5027JUrVzr7py+OjCcN7b+37/F+fMHU7r9nzx6nT4Nf3r9g3v5ktr/B75dOmsEKFizofDEHP6ZOnTqnPId3X7ly5SxeqG+D3rO+ffumW4TwjjvucPoq6LjmzJnjhFZ9vp988sm4P0b1r7n88sud/fvpp5/s/vvvtx49ejgntQIFCvjuPXzttdec/is65mCJ8B6ezOS8EK7vzaweowB06NAhp0/d6SLcIJU6h+mE//XXX6e7/dZbb029rCSujpwXX3yx86VUr149i3f60vQ0a9bMCTs62b/zzjt5+uOJRy+//LJzvAoyfnn/kpn+O7766qudTtRjx45Nd9/dd9+d7nOtk82f/vQnpzNovE/rf+2116b7TGr/9VlUbY4+m34zbtw4p8ZYnYIT7T0cksV5Id7RLJVHqurXfxoZe4nreuXKlS1RDB061D755BObOXOmVa9ePeRjFQ5k7dq1zlbHmdnxe/fFG/23cfbZZzv7r/1TU45qO7J6/xLl+DZu3GhffPGF3XLLLb59/7z9CfX3pu2OHTvS3a+qfo2+SaT31As2el+nTZuWrtYmq/dVx7lhw4aEOUaPmov1XRr8mfTDeyhfffWVU1Oa3d9lPL6HQ7M4L4TrezOrx+izntd/PAk3eaSk3apVK5s+fXq6ajxdb9euncU7/UeoD/DEiRNtxowZp1SBZmbp0qXOVjUAouNcsWJFui8j78u4cePGFm80nFS1Ftp/vXeFChVK9/7pi0h9crz3L1GO75VXXnGq8jX00q/vnz6f+kIMfr9Uha1+GMHvl7501S/Ao8+2/i69YKfHaCivAkTwMavpMh6aM7xgo75iCqzqk5Edva/qk+I158T7MQb7+eefnT43wZ/JRH8Pg2tT9T3TvHnzhHkPA9mcF8L1vanHBD+H95iwnDvz3CUZzlBwjdZ49dVXnV7+t956qzMUPLiXeLwaPHiwM6x21qxZ6YYjHjx40Ll/7dq1zlBFDfVbv3594MMPPwzUrVs30LFjx1OG/F166aXOsEEN4zvzzDPjZqj0Pffc4xyf9v+bb75xhiZqSKJGAHhDGjXMccaMGc5xtmvXzimJcnzeCD0dg0ZSBEvE92/fvn3O0FEVfUU9+eSTzmVvpJCGguvvS8eyfPlyZxRKZkPBW7RoEZg/f37g66+/Dpx11lnphhFrtIeG2F5//fXOcFf9DWtIarSGEYc6xqNHjzrD26tXr+68H8F/l94okzlz5jijbHS/hhaPHz/eec9uuOGGuDjGUMen+/7yl784o2r0mfziiy8CLVu2dN6jw4cP++I9DB7KrX3SKKGM4vk9HJzNeSFc35veUPB7773XGW313HPPMRQ83mh8vt5ozXejoeGamyER6I8ys6I5DmTTpk3OibB8+fJOgNNcE/ogBs+TIhs2bAj06NHDmYNBwUGB4tixY4F4oKGFVapUcd6batWqOdd10vfopHjbbbc5Qy71h/aHP/zB+UNOlOOTzz77zHnf1qxZk+72RHz/ND9PZp9JDR/2hoP//e9/d770dUwXX3zxKcf922+/OSfCkiVLOkNPb7zxRudkFExz5FxwwQXOc+hzodAUD8eoE35Wf5fe3EWLFi0KnH/++c4JqGjRooFGjRoFHn300XThIJbHGOr4dILUCU8nOg0n1nBozcOU8Z/BRH4PPQoh+ptSSMkont9Dy+a8EM7vTf0ezz33XOf7Wf94Bb9GXuT734EAAAD4An1uAACArxBuAACArxBuAACArxBuAACArxBuAACArxBuAACArxBuAACArxBuAACArxBuACS9fPny2aRJk2K9GwDChHADIKYGDBjghIuMpXv37rHeNQAJqmCsdwAAFGS0qnmwIkWKxGx/ACQ2am4AxJyCTOXKldOVcuXKOfepFmfs2LHWo0cPK1asmNWtW9fee++9dD+/YsUK69Kli3P/GWecYbfeeqvt378/3WPGjRtnTZo0cV6rSpUqNnTo0HT3//rrr/aHP/zBihcvbmeddZZ99NFHUThyAJFAuAEQ9/7+97/bFVdcYcuWLbN+/frZtddea6tXr3buO3DggHXr1s0JQwsWLLB3333Xvvjii3ThReFoyJAhTuhREFJwqV+/frrXePjhh+3qq6+25cuXW8+ePZ3X2bVrV9SPFUAYhGVtcQA4Tf379w8UKFAgUKJEiXTlkUcece7X19SgQYPS/cz5558fGDx4sHP5pZdeCpQrVy6wf//+1PsnT54cyJ8/f2Dbtm3O9apVqwYeeOCBLPdBr/Hggw+mXtdz6bYpU6aE/XgBRB59bgDE3EUXXeTUrgQrX7586uV27dqlu0/Xly5d6lxWDU7z5s2tRIkSqfd36NDBTp48aWvWrHGatbZs2WIXX3xxyH1o1qxZ6mU9V+nSpW3Hjh15PjYA0Ue4ARBzChMZm4nCRf1wcqJQoULprisUKSABSDz0uQEQ9+bNm3fK9UaNGjmXtVVfHPW98XzzzTeWP39+a9CggZUqVcpq165t06dPj/p+A4gNam4AxNyRI0ds27Zt6W4rWLCgVahQwbmsTsKtW7e2Cy64wN544w379ttv7eWXX3buU8ff4cOHW//+/e2hhx6ynTt32u23327XX3+9VapUyXmMbh80aJBVrFjRGXW1b98+JwDpcQD8h3ADIOamTp3qDM8OplqX77//PnUk04QJE+y2225zHvfWW29Z48aNnfs0dPuzzz6zO++809q0aeNc18iqJ598MvW5FHwOHz5sY8aMsb/85S9OaLryyiujfJQAoiWfehVH7dUAIJfU92XixInWp0+fWO8KgARBnxsAAOArhBsAAOAr9LkBENdoOQeQW9TcAAAAXyHcAAAAXyHcAAAAXyHcAAAAXyHcAAAAXyHcAAAAXyHcAAAAXyHcAAAA85P/D2sKJrzKqwxHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"10\"}--> \n",
       "         ✓ [10 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Veamos la primera secuencia en el test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode el input y el target\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Init el hidden state con ceros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Hacemos el pase forward para evalular nuestra secuencia\n",
    "outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print(\"Secuencia Input:\")\n",
    "print(inputs)\n",
    "\n",
    "print(\"Secuencia Target:\")\n",
    "print(targets)\n",
    "\n",
    "print(\"Secuencia Predicha:\")\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Graficamos la perdida\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()\n",
    "\n",
    "with tick.marks(10):        \n",
    "    assert compare_lists_by_percentage(targets, [idx_to_word[np.argmax(output)] for output in outputs], 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c223d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T07:01:10.253203Z",
     "start_time": "2023-07-30T07:01:10.243032Z"
    }
   },
   "source": [
    "#### Preguntas\n",
    "\n",
    "Ya hemos visto el funcionamiento general de nuestra red RNN, viendo las gráficas de arriba, **responda** lo siguiente dentro de esta celda\n",
    "\n",
    "* ¿Qué interpretación le da a la separación de las graficas de training y validation?\n",
    "La separación indica que el modelo está aprendiendo a reducir el error sobre los datos de entrenamiento, pero hay discrepancia al aplicarse sobre los datos de validación. Esa separación lo que demuestra o sugiere es que el modelo tiene overfitting y no generaliza tan bien a datos nuevos. La separación entre las primeras 500 y 750 épocas donde la validación se estabiliza mientras qué el entrenamiento sigue bajando ligeramente. \n",
    "* ¿Cree que es un buen modelo basado solamente en el loss?\n",
    "Creo que es un buen modelo, sin embargo veo aspectos negativos, la verdad quisiera hacer los siguientes modelos para comparar y ver si es inferior a ellos, pero a este punto del laboratorio lo que veo principalmente es: \n",
    "  - Aspectos positivos: ambas curvas disminuyen significativamente en las primeras épocas, lo que demuestra que la red logra aprender patrones relevantes.\n",
    "  - Aspectos negativos: la brecha entre entrenamiento y validación sugiere overfitting.\n",
    "\n",
    "* ¿Cómo deberían de verse esas gráficas en un modelo ideal?\n",
    "La curva de training loss y validation loss decrecen juntas y se mantienen cercanas durante el entrenamiento. Ambas se estabilizan sin una diferencia significativa entre ellas. No debería haber un aumento en la validación (validation loss) hacia las últimas épocas. Visualmente, esto se vería como dos curvas paralelas y casi solapadas, indicando que el modelo generaliza bien y no presenta overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0001d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33717eb5a11832cbcf3afe049aa819f2",
     "grade": false,
     "grade_id": "cell-3b641dbd0cd4a7fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Parte 2 - Construyendo una Red Neuronal LSTM \n",
    "\n",
    "**Créditos:** La segunda parte de este laboratorio está tomado y basado en uno de los laboratorios dados dentro del curso de \"Deep Learning\" de Jes Frellsen (DeepLearningDTU)\n",
    "\n",
    "\n",
    "Consideren leer el siguiente blog para mejorar el entendimiento de este tema: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "La RNN estándar enfrenta un problema de gradientes que desaparecen, lo que dificulta la retención de memoria en secuencias más largas. Para hacer frente a estos desafíos, se introdujeron algunas variantes. \n",
    "\n",
    "Los dos tipos principales son la celda de memoria a corto plazo (LSTM) y la unidad recurrente cerrada (GRU), las cuales demuestran una capacidad mejorada para conservar y utilizar la memoria en pasos de tiempo posteriores. \n",
    "\n",
    "En este ejercicio, nuestro enfoque estará en LSTM, pero los principios aprendidos aquí también se pueden aplicar fácilmente para implementar GRU.\n",
    "\n",
    "Recordemos una de las imagenes que vimos en clase\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Savvas-Varsamopoulos/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg\" alt=\"LSTM\" />\n",
    "\n",
    "*Crédito de imagen al autor, imagen tomada de \"Designing neural network based decoders for surface codes\" de Savvas Varsamopoulos*\n",
    "\n",
    "\n",
    "Recordemos que la \"celula\" de LST contiene tres tipos de gates, input, forget y output gate. La salida de una unidad LSTM está calculada por las siguientes funciones, donde  $\\sigma = softmax$. Entonces tenemos la input gate $i$, la forget gate $f$ y la output gate $o$\n",
    "\n",
    "* $i = \\sigma ( W^i [h_{t-1}, x_t])$\n",
    "* $f = \\sigma ( W^f [h_{t-1},x_t])$\n",
    "* $o = \\sigma ( W^o [h_{t-1},x_t])$\n",
    "\n",
    "Donde $W^i, W^f, W^o$ son las matrices de pesos aplicada a cada aplicadas a una matriz contatenada $h_{t-1}$ (hidden state vector) y $x_t$ (input vector) para cada respectiva gate $h_{t-1}$, del paso previo junto con el input actual $x_t$ son usados para calcular una memoria candidata $g$\n",
    "\n",
    "* $g = tanh( W^g [h_{t-1}, x_t])$\n",
    "\n",
    "El valor de la memoria $c_t$ es actualizada como\n",
    "\n",
    "$c_t = c_{t-1} \\circ f + g \\circ i$\n",
    "\n",
    "donde $c_{t-1}$ es la memoria previa, y $\\circ$ es una multiplicacion element-wise (recuerden que este tipo de multiplicación en numpy es con *)\n",
    "\n",
    "La salida $h_t$ es calculada como\n",
    "\n",
    "$h_t = tanh(c_t) \\circ o$\n",
    "\n",
    "y este se usa para tanto la salida del paso como para el siguiente paso, mientras $c_t$ es exclusivamente enviado al siguiente paso. Esto hace $c_t$ una memoria feature, y no es usado directamente para caluclar la salida del paso actual.\n",
    "\n",
    "### Iniciando una Red LSTM\n",
    "\n",
    "De forma similar a lo que hemos hecho antes, necesitaremos implementar el paso forward, backward y un ciclo de entrenamiento. Pero ahora usaremos LSTM con NumPy. Más adelante veremos como es que esto funciona con PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a2c856b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.553871Z",
     "start_time": "2023-08-05T23:32:05.538285Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62b5aae14a3dc0ee3dbca646ce607e19",
     "grade": false,
     "grade_id": "cell-07f509efcc1a3ccb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed_)\n",
    "\n",
    "# Tamaño del hidden state concatenado más el input\n",
    "z_size = hidden_size + vocab_size \n",
    "\n",
    "def init_lstm(hidden_size, vocab_size, z_size):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    Init LSTM\n",
    "    \n",
    "    Args:\n",
    "     hidden_size: Dimensiones del hidden state\n",
    "     vocab_size: Dimensiones de nuestro vocabulario\n",
    "     z_size: Dimensiones del input concatenado \n",
    "    \"\"\"\n",
    "\n",
    "    # Aprox 1 linea para empezar la matriz de pesos de la forget gate\n",
    "    # Recuerden que esta debe empezar con numeros aleatorios\n",
    "    # W_f = np.random.randn\n",
    "    # YOUR CODE HERE\n",
    "    W_f = np.random.randn(hidden_size, z_size)\n",
    "    \n",
    "    # Bias del forget gate\n",
    "    b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Aprox 1 linea para empezar la matriz de pesos de la input gate\n",
    "    # Recuerden que esta debe empezar con numeros aleatorios\n",
    "    # YOUR CODE HERE\n",
    "    W_i = np.random.randn(hidden_size, z_size)\n",
    "    \n",
    "    # Bias para input gate\n",
    "    b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Aprox 1 linea para empezar la matriz de pesos para la memoria candidata\n",
    "    # Recuerden que esta debe empezar con numeros aleatorios\n",
    "    # YOUR CODE HERE\n",
    "    W_g = np.random.randn(hidden_size, z_size)\n",
    "    \n",
    "    # Bias para la memoria candidata\n",
    "    b_g = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Aprox 1 linea para empezar la matriz de pesos para la output gate\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    W_o = np.random.randn(hidden_size, z_size)\n",
    "    \n",
    "    # Bias para la output gate\n",
    "    b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "\n",
    "    # Aprox 1 linea para empezar la matriz que relaciona el hidden state con el output\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    W_v = np.random.randn(vocab_size, hidden_size)\n",
    "    \n",
    "    # Bias\n",
    "    b_v = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Init pesos ortogonalmente (https://arxiv.org/abs/1312.6120)\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
    "\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "412a27b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.569529Z",
     "start_time": "2023-08-05T23:32:05.553871Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f54f80a804b45836347ca5928b1902b0",
     "grade": true,
     "grade_id": "cell-1145b5a61bdcda0f",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(5):        \n",
    "    assert check_hash(params[0], ((50, 54), -28071.583543573637))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[1], ((50, 54), -6337.520066952928))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[2], ((50, 54), -13445.986473992281))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[3], ((50, 54), 2276.1116210911564))\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert check_hash(params[4], ((4, 50), -201.28961326044097))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5035e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e55ee118bbd693b1c9f42414a5af868",
     "grade": false,
     "grade_id": "cell-c69b9a17df9ca940",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Forward\n",
    "\n",
    "Vamos para adelante con LSTM, al igual que previamente necesitamos implementar las funciones antes mencionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8a59a4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.585110Z",
     "start_time": "2023-08-05T23:32:05.569529Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b388082beee631c97ae27b131c638ee0",
     "grade": false,
     "grade_id": "cell-1277d0634231924c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: Input data en el paso \"t\", shape (n_x, m)\n",
    "    h_prev: Hidden state en el paso \"t-1\", shape (n_a, m)\n",
    "    C_prev: Memoria en el paso \"t-1\", shape (n_a, m)\n",
    "    p: Lista con pesos y biases, contiene:\n",
    "                        W_f:  Pesos de la forget gate, shape (n_a, n_a + n_x)\n",
    "                        b_f: Bias de la forget gate, shape (n_a, 1)\n",
    "                        W_i: Pesos de la update gate, shape (n_a, n_a + n_x)\n",
    "                        b_i: Bias de la update gate, shape (n_a, 1)\n",
    "                        W_g: Pesos de la primer \"tanh\", shape (n_a, n_a + n_x)\n",
    "                        b_g: Bias de la primer \"tanh\", shape (n_a, 1)\n",
    "                        W_o: Pesos de la output gate, shape (n_a, n_a + n_x)\n",
    "                        b_o: Bias de la output gate, shape (n_a, 1)\n",
    "                        W_v: Pesos de la matriz que relaciona el hidden state con el output, shape (n_v, n_a)\n",
    "                        b_v: Bias que relaciona el hidden state con el output, shape (n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s: Lista de tamaño m conteniendo los calculos de cada paso forward\n",
    "    outputs: Predicciones en el paso \"t\", shape (n_v, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validar las dimensiones\n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "\n",
    "    # Desempacar los parametros\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    # Listas para calculos de cada componente en LSTM\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Agregar los valores iniciales \n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        x_s.append(x)\n",
    "        # Aprox 1 linea para concatenar el input y el hidden state\n",
    "        # z = np.row.stack(...)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        z = np.row_stack([h_prev, x])\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # Aprox 1 linea para calcular el forget gate\n",
    "        # Hint: recuerde usar sigmoid\n",
    "        # f = \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        f = sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # Calculo del input gate\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # Calculo de la memoria candidata\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # Aprox 1 linea para calcular el estado de la memoria\n",
    "        # C_prev = \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        C_prev = f * C_prev + g * i\n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # Aprox 1 linea para el calculo de la output gate\n",
    "        # Hint: recuerde usar sigmoid\n",
    "        # o = \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # Calculate hidden state\n",
    "        # Aprox 1 linea para el calculo del hidden state\n",
    "        # h_prev =\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        h_prev = o*tanh(C_prev)\n",
    "        \n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # Calcular logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "        \n",
    "        # Calculo de output (con softmax)\n",
    "        output = softmax(v)\n",
    "        output_s.append(output)\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "71f758df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.600776Z",
     "start_time": "2023-08-05T23:32:05.585110Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94b91568cf22e1f75709bfe774316fd7",
     "grade": true,
     "grade_id": "cell-4c878e36c9c270ab",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia Input:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Secuencia Target:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Secuencia Predicha:\n",
      "['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS', 'EOS', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtener la primera secuencia para probar\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode del input y target\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Init hidden state con ceros\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "\n",
    "print(\"Secuencia Input:\")\n",
    "print(inputs)\n",
    "\n",
    "print(\"Secuencia Target:\")\n",
    "print(targets)\n",
    "\n",
    "print(\"Secuencia Predicha:\")\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "with tick.marks(5):        \n",
    "    assert check_hash(outputs, ((22, 4, 1), 980.1651308051631))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6473816",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a336c2502c28403858fffbc0ec095bb2",
     "grade": false,
     "grade_id": "cell-f1fb26540d33e61b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Backward\n",
    "\n",
    "Ahora de reversa, al igual que lo hecho antes, necesitamos implementar el paso de backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a753b92e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.616358Z",
     "start_time": "2023-08-05T23:32:05.600776Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "149234786a31e8903430dfe2ff9b25aa",
     "grade": false,
     "grade_id": "cell-8500a307f5192db0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z: Input concatenado como una lista de tamaño m.\n",
    "    f: Calculos del forget gate como una lista de tamaño m.\n",
    "    i: Calculos del input gate como una lista de tamaño m.\n",
    "    g: Calculos de la memoria candidata como una lista de tamaño m.\n",
    "    C: Celdas estado como una lista de tamaño m+1.\n",
    "    o: Calculos del output gate como una lista de tamaño m.\n",
    "    h: Calculos del Hidden State como una lista de tamaño m+1.\n",
    "    v: Calculos del logit como una lista de tamaño m.\n",
    "    outputs: Salidas como una lista de tamaño m.\n",
    "    targets: Targets como una lista de tamaño m.\n",
    "    p: Lista con pesos y biases, contiene:\n",
    "                        W_f:  Pesos de la forget gate, shape (n_a, n_a + n_x)\n",
    "                        b_f: Bias de la forget gate, shape (n_a, 1)\n",
    "                        W_i: Pesos de la update gate, shape (n_a, n_a + n_x)\n",
    "                        b_i: Bias de la update gate, shape (n_a, 1)\n",
    "                        W_g: Pesos de la primer \"tanh\", shape (n_a, n_a + n_x)\n",
    "                        b_g: Bias de la primer \"tanh\", shape (n_a, 1)\n",
    "                        W_o: Pesos de la output gate, shape (n_a, n_a + n_x)\n",
    "                        b_o: Bias de la output gate, shape (n_a, 1)\n",
    "                        W_v: Pesos de la matriz que relaciona el hidden state con el output, shape (n_v, n_a)\n",
    "                        b_v: Bias que relaciona el hidden state con el output, shape (n_v, 1)\n",
    "    Returns:\n",
    "    loss: crossentropy loss para todos los elementos del output\n",
    "    grads: lista de gradientes para todos los elementos en p\n",
    "    \"\"\"\n",
    "\n",
    "    # Desempacar parametros\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "\n",
    "    # Init gradientes con cero\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Setear la proxima unidad y hidden state con ceros\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    # Para la perdida\n",
    "    loss = 0\n",
    "    \n",
    "    # Iteramos en reversa los outputs\n",
    "    for t in reversed(range(len(outputs))):\n",
    "        \n",
    "        # Aprox 1 linea para calcular la perdida con cross entropy\n",
    "        # loss += ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Obtener el hidden state del estado previo\n",
    "        C_prev= C[t-1]\n",
    "        \n",
    "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "        # Calculo de las derivadas en relacion del hidden state al output gate\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        # Aprox 1 linea para actualizar la gradiente de la relacion del hidden-state al output gate\n",
    "        # W_v_d += \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        b_v_d += dv\n",
    "\n",
    "        # Calculo de la derivada del hidden state y el output gate\n",
    "        dh = np.dot(W_v.T, dv)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        # Aprox 1 linea para calcular la derivada del output\n",
    "        # do = ..\n",
    "        # Hint: Recuerde multiplicar por el valor previo de do (el de arriba)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Actualizacion de las gradientes con respecto al output gate\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # Calculo de las derivadas del estado y la memoria candidata g\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        # Aprox 1 linea de codigo para terminar el calculo de dg\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Actualización de las gradientes con respecto de la mem candidata\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        # Calculo de la derivada del input gate y la actualización de sus gradientes\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        # Aprox 2 lineas para el calculo de los pesos y bias del input gate\n",
    "        # W_i_d += \n",
    "        # b_i_d +=\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Calculo de las derivadas del forget gate y actualización de sus gradientes\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        # Aprox 2 lineas para el calculo de los pesos y bias de la forget gate\n",
    "        # W_f_d += \n",
    "        # b_f_d +=\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Calculo de las derivadas del input y la actualizacion de gradientes del hidden state previo\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hidden_size, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    \n",
    "    # Recorte de gradientes\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58def9bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:32:05.631979Z",
     "start_time": "2023-08-05T23:32:05.616358Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f17904c9bbc54f6acdd9e59ead87adc0",
     "grade": true,
     "grade_id": "cell-baf03f239d56e288",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Realizamos un backward pass para probar\n",
    "loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "\n",
    "print(f\"Perdida obtenida:{loss}\")\n",
    "\n",
    "with tick.marks(5):        \n",
    "    assert(check_scalar(loss, '0x53c34f25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739dbcd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d33c26c1ed061d46ae3bb649a1d8f4e0",
     "grade": false,
     "grade_id": "cell-68df4c065c8367d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "Ahora intentemos entrenar nuestro LSTM básico. Esta parte es muy similar a lo que ya hicimos previamente con la RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250482a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:34:07.095962Z",
     "start_time": "2023-08-05T23:32:05.631979Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1882170a6b982a00cd873c6d50cc1e09",
     "grade": false,
     "grade_id": "cell-cf9622776d252627",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyper parametros\n",
    "num_epochs = 500\n",
    "\n",
    "# Init una nueva red\n",
    "z_size = hidden_size + vocab_size # Tamaño del hidden concatenado + el input\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
    "\n",
    "# Init hidden state como ceros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Perdida\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Iteramos cada epoca\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Perdidas\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    # Para cada secuencia en el validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode el inpyt y el target\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # Init hidden state y la unidad de estado como ceros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        # Backward \n",
    "        loss, _ = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "        # Actualizacion de la perdida\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # Para cada secuencia en el training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode el inpyt y el target\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # Init hidden state y la unidad de estado como ceros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        # Backward\n",
    "        loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "        # Actualización de parametros\n",
    "        params = update_parameters(params, grads, lr=1e-1)\n",
    "        \n",
    "        # Actualizacion de la perdida\n",
    "        epoch_training_loss += loss\n",
    "                \n",
    "    # Guardar la perdida para ser graficada\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Mostrar la perdida cada 5 epocas\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a11a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:34:07.236710Z",
     "start_time": "2023-08-05T23:34:07.095962Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5db6b37684f2913ca50ec8a4c8f5981f",
     "grade": false,
     "grade_id": "cell-7814184dd4823fac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Obtener la primera secuencia del test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode el input y el target\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Init hidden state como ceros\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward \n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "\n",
    "print(\"Secuencia Input:\")\n",
    "print(inputs)\n",
    "\n",
    "print(\"Secuencia Target:\")\n",
    "print(targets)\n",
    "\n",
    "print(\"Secuencia Predicha:\")\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Graficar la perdida en training y validacion\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8692424",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4c179da7bfac5c35f0bc42867fe83cf",
     "grade": false,
     "grade_id": "cell-a8b4db0d7c0dd6cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Preguntas\n",
    "\n",
    "**Responda** lo siguiente dentro de esta celda\n",
    "\n",
    "* ¿Qué modelo funcionó mejor? ¿RNN tradicional o el basado en LSTM? ¿Por qué?\n",
    "* Observen la gráfica obtenida arriba, ¿en qué es diferente a la obtenida a RNN? ¿Es esto mejor o peor? ¿Por qué?\n",
    "* ¿Por qué LSTM puede funcionar mejor con secuencias largas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc991e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7b6dbdd11cf95d69a352306b879c05b",
     "grade": false,
     "grade_id": "cell-70b696ca36c0804a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Parte 3 - Red Neuronal LSTM con PyTorch \n",
    "\n",
    "Ahora que ya hemos visto el funcionamiento paso a paso de tanto RNN tradicional como LSTM. Es momento de usar PyTorch. Para esta parte usaremos el mismo dataset generado al inicio. Así mismo, usaremos un ciclo de entrenamiento similar al que hemos usado previamente. \n",
    "\n",
    "En la siguiente parte (sí, hay una siguiente parte &#x1F913;) usaremos otro tipo de dataset más formal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18938b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:34:07.267949Z",
     "start_time": "2023-08-05T23:34:07.236710Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee2d3fa1a4e9d2426203334a38a4af8e",
     "grade": false,
     "grade_id": "cell-311fc1fe42eca687",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Aprox 1-3 lineas de codigo para declarar una capa LSTM\n",
    "        # self.lstm = \n",
    "        # Hint: Esta tiene que tener el input_size del tamaño del vocabulario,\n",
    "        #     debe tener 50 hidden states (hidden_size)\n",
    "        #     una layer\n",
    "        #     y NO (False) debe ser bidireccional \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Layer de salida (output)\n",
    "        self.l_out = nn.Linear(in_features=50,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN regresa el output y el ultimo hidden state\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Aplanar la salida para una layer feed forward\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # layer de output \n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c896f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:00.753913Z",
     "start_time": "2023-08-05T23:34:07.267949Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad63c124dd865aa9b8c0da08852718ad",
     "grade": false,
     "grade_id": "cell-04486b8d9ade1533",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyper parametros\n",
    "num_epochs = 500\n",
    "\n",
    "# Init una nueva red\n",
    "net = Net()\n",
    "\n",
    "# Aprox 2 lineas para definir la función de perdida y el optimizador\n",
    "# criterion = # Use CrossEntropy\n",
    "# optimizer = # Use Adam con lr=3e-4\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Perdida\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Iteramos cada epoca\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Perdidas\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    # NOTA 1\n",
    "    net.eval()\n",
    "        \n",
    "    # Para cada secuencia en el validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode el inpyt y el target\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convertir el input a un tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convertir el target a un tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Aprox 1 linea para el Forward \n",
    "        # outputs = \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Aprox 1 linea para calcular la perdida\n",
    "        # loss =\n",
    "        # Hint: Use el criterion definido arriba\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Actualizacion de la perdida\n",
    "        epoch_validation_loss += loss.detach().numpy()\n",
    "    \n",
    "    # NOTA 2\n",
    "    net.train()\n",
    "    \n",
    "    # Para cada secuencia en el training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode el inpyt y el target\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convertir el input a un tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convertir el target a un tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Aprox 1 linea para el Forward \n",
    "        # outputs = \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Aprox 1 linea para calcular la perdida\n",
    "        # loss =\n",
    "        # Hint: Use el criterion definido arriba\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Aprox 3 lineas para definir el backward\n",
    "        # optimizer.\n",
    "        # loss.\n",
    "        # optimizer.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Actualizacion de la perdida\n",
    "        epoch_training_loss += loss.detach().numpy()\n",
    "        \n",
    "    # Guardar la perdida para ser graficada\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Mostrar la perdida cada 5 epocas\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca199e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:00.769534Z",
     "start_time": "2023-08-05T23:35:00.753913Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18a505ffb2aa6222c3894bc5fee82e02",
     "grade": true,
     "grade_id": "cell-acfe6153f9006b27",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with tick.marks(5):        \n",
    "    assert compare_numbers(new_representation(training_loss[-1]), \"3c3d\", '0x1.28f5c28f5c28fp-2')\n",
    "    \n",
    "with tick.marks(5):        \n",
    "    assert compare_numbers(new_representation(validation_loss[-1]), \"3c3d\", '0x1.28f5c28f5c28fp-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561162c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:00.925207Z",
     "start_time": "2023-08-05T23:35:00.769534Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5847ed5bbead7e432e5e12d4eb6114a3",
     "grade": false,
     "grade_id": "cell-3e1bfd6f4ff9568e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Obtener la primera secuencia del test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode el input y el target\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_idx = [word_to_idx[word] for word in targets]\n",
    "\n",
    "# Convertir el input a un tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convertir el target a un tensor\n",
    "targets_idx = torch.LongTensor(targets_idx)\n",
    "\n",
    "\n",
    "# Aprox 1 linea para el Forward \n",
    "# outputs = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Secuencia Input:\")\n",
    "print(inputs)\n",
    "\n",
    "print(\"Secuencia Target:\")\n",
    "print(targets)\n",
    "\n",
    "print(\"Secuencia Predicha:\")\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Graficar la perdida en training y validacion\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03eeae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f80da25b1e5ffd177becd68eb2c2dde2",
     "grade": false,
     "grade_id": "cell-1fb0f402aab24ee3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Preguntas\n",
    "\n",
    "**Responda** lo siguiente dentro de esta celda\n",
    "\n",
    "* Compare las graficas obtenidas en el LSTM \"a mano\" y el LSTM \"usando PyTorch, ¿cuál cree que es mejor? ¿Por qué?\n",
    "* Compare la secuencia target y la predicha de esta parte, ¿en qué parte falló el modelo?\n",
    "* ¿Qué sucede en el código donde se señala \"NOTA 1\" y \"NOTA 2\"? ¿Para qué son necesarias estas líneas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ccbc27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "772e173eeac39b0919121141a48d2484",
     "grade": false,
     "grade_id": "cell-5bb7d8b7aa5b0eba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Parte 4 - Segunda Red Neuronal LSTM con PyTorch \n",
    "\n",
    "Para esta parte será un poco menos guiada, por lo que se espera que puedan generar un modelo de Red Neuronal con LSTM para solventar un problema simple. Lo que se evaluará es la métrica final, y solamente se dejarán las generalidades de la implementación. El objetivo de esta parte, es dejar que ustedes exploren e investiguen un poco más por su cuenta. \n",
    "\n",
    "En este parte haremos uso de las redes LSTM pero para predicción de series de tiempo. Entonces lo que se busca es que dado un mes y un año, se debe predecir el número de pasajeros en unidades de miles. Los datos a usar son de 1949 a 1960.\n",
    "\n",
    "Basado del blog \"LSTM for Time Series Prediction in PyTorch\" de Adrian Tam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4e03a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:00.949969Z",
     "start_time": "2023-08-05T23:35:00.927201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seed all\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "torch.manual_seed(seed_)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_)\n",
    "    torch.cuda.manual_seed_all(seed_)  # Multi-GPU.\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b161e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:04.694521Z",
     "start_time": "2023-08-05T23:35:00.951962Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\"\n",
    "dataset = pd.read_csv(url_data)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39147fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:04.817904Z",
     "start_time": "2023-08-05T23:35:04.694521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dibujemos la serie de tiempo\n",
    "time_series = dataset[[\"Passengers\"]].values.astype('float32')\n",
    "\n",
    "plt.plot(time_series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0a974",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b571f3d3e711cd00704160b9076470c",
     "grade": false,
     "grade_id": "cell-62ab455036fa4a55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Esta serie de tiempo comprende 144 pasos de tiempo. El gráfico indica claramente una tendencia al alza y hay patrones periódicos en los datos que corresponden al período de vacaciones de verano. Por lo general, se recomienda \"eliminar la tendencia\" de la serie temporal eliminando el componente de tendencia lineal y normalizándolo antes de continuar con el procesamiento. Sin embargo, por simplicidad de este ejercicios, vamos a omitir estos pasos.\n",
    "\n",
    "Ahora necesitamos dividir nuestro dataset en training, validation y test set. A diferencia de otro tipo de datasets, cuando se trabaja en este tipo de proyectos, la división se debe hacer sin \"revolver\" los datos. Para esto, podemos hacerlo con NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a179c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:04.840674Z",
     "start_time": "2023-08-05T23:35:04.817904Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07cb1e706347a5e56eac2633b37bcaf1",
     "grade": false,
     "grade_id": "cell-35af372f0bf820a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# En esta ocasion solo usaremos train y test, validation lo omitiremos para simpleza del ejercicio\n",
    "# NO CAMBIEN NADA DE ESTA CELDA POR FAVOR\n",
    "p_train=0.8\n",
    "p_test=0.2\n",
    "\n",
    "# Definimos el tamaño de las particiones\n",
    "num_train = int(len(time_series)*p_train)\n",
    "num_test = int(len(time_series)*p_test)\n",
    "\n",
    "\n",
    "# Dividir las secuencias en las particiones\n",
    "train = time_series[:num_train]\n",
    "test = time_series[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b657bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40c1e60513e029a06d25435af49dad3a",
     "grade": false,
     "grade_id": "cell-ece3e13c7a8ed477",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "El aspecto más complicado es determinar el método por el cual la red debe predecir la serie temporal. Por lo general, la predicción de series temporales se realiza en función de una ventana. En otras palabras, recibe datos del tiempo t1 al t2, y su tarea es predecir para el tiempo t3 (o más adelante). El tamaño de la ventana, denotado por w, dicta cuántos datos puede considerar el modelo al hacer la predicción. Este parámetro también se conoce como **look back period** (período retrospectivo).\n",
    "\n",
    "Entonces, creemos una función para obtener estos datos, dado un look back period. Además, debemos asegurarnos de transformar estos datos a tensores para poder ser usados con PyTorch.\n",
    "\n",
    "Esta función está diseñada para crear ventanas en la serie de tiempo mientras predice un paso de tiempo en el futuro inmediato. Su propósito es convertir una serie de tiempo en un tensor con dimensiones (muestras de ventana, pasos de tiempo, características). Dada una serie de tiempo con t pasos de tiempo, puede producir aproximadamente (t - ventana + 1) ventanas, donde \"ventana\" denota el tamaño de cada ventana. Estas ventanas pueden comenzar desde cualquier paso de tiempo dentro de la serie de tiempo, siempre que no se extiendan más allá de sus límites.\n",
    "\n",
    "Cada ventana contiene múltiples pasos de tiempo consecutivos con sus valores correspondientes, y cada paso de tiempo puede tener múltiples características. Sin embargo, en este conjunto de datos específico, solo hay una función disponible.\n",
    "\n",
    "La elección del diseño garantiza que tanto la \"característica\" como el \"objetivo\" tengan la misma forma. Por ejemplo, para una ventana de tres pasos de tiempo, la \"característica\" corresponde a la serie de tiempo de t-3 a t-1, y el \"objetivo\" cubre los pasos de tiempo de t-2 a t. Aunque estamos principalmente interesados en predecir t+1, la información de t-2 a t es valiosa durante el entrenamiento.\n",
    "\n",
    "Es importante tener en cuenta que la serie temporal de entrada se representa como una matriz 2D, mientras que la salida de la función `create_timeseries_dataset()` será un tensor 3D. Para demostrarlo, usemos lookback=1 y verifiquemos la forma del tensor de salida en consecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6e2f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:04.862500Z",
     "start_time": "2023-08-05T23:35:04.842686Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_timeseries_dataset(dataset, lookback):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - lookback):\n",
    "        feature = dataset[i : i + lookback]\n",
    "        target = dataset[i + 1 : i + lookback + 1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "# EL VALOR DE LB SÍ LO PUEDEN CAMBIAR SI LO CONSIDERAN NECESARIO\n",
    "lb = 4\n",
    "X_train, y_train = create_timeseries_dataset(train, lookback=lb)\n",
    "#X_validation, y_validation = create_timeseries_dataset(validation, lookback=lb)\n",
    "X_test, y_test = create_timeseries_dataset(test, lookback=lb)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "#print(X_validation.shape, y_validation.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f894c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d10291404d48c7939620e98bdf5c78c9",
     "grade": false,
     "grade_id": "cell-23fc69181d7a7cd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora necesitamos crear una clase que definirá nuestro modelo de red neuronal con LSTM. Noten que acá solo se dejaran las firmas de las funciones necesarias, ustedes deberán decidir que arquitectura con LSTM implementar, con la finalidad de superar cierto threshold de métrica de desempeño mencionado abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5df7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:35:04.893730Z",
     "start_time": "2023-08-05T23:35:04.862500Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51bcc393e21e6cbb4e8535556d11e975",
     "grade": false,
     "grade_id": "cell-f0f68d3f484736df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# NOTA: Moví el numero de iteraciones para que no se borre al ser evaluado\n",
    "# Pueden cambiar el número de epocas en esta ocasión con tal de llegar al valor de la metrica de desempeño\n",
    "# n_epochs = 3000\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "class CustomModelLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca183d4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a4476b61104b249dbdf1098ff92545f",
     "grade": false,
     "grade_id": "cell-e023e0bb22dd42ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "La función nn.LSTM() produce una tupla como salida. El primer elemento de esta tupla consiste en los hidden states generados, donde cada paso de tiempo de la entrada tiene su correspondiente hidden state. El segundo elemento contiene la memoria y los hidden states de la unidad LSTM, pero no se usan en este contexto particular.\n",
    "\n",
    "La capa LSTM se configura con la opción `batch_first=True` porque los tensores de entrada se preparan en la dimensión de (muestra de ventana, pasos de tiempo, características). Con esta configuración, se crea un batch tomando muestras a lo largo de la primera dimensión.\n",
    "\n",
    "Para generar un único resultado de regresión, la salida de los estados ocultos se procesa aún más utilizando una capa fully connected. Dado que la salida de LSTM corresponde a un valor para cada paso de tiempo de entrada, se debe seleccionar solo la salida del último paso de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae7532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:36:19.729245Z",
     "start_time": "2023-08-05T23:35:04.893730Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce72799bead411086daec37631d789e",
     "grade": true,
     "grade_id": "cell-d106920d76b987cc",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# NOTEN QUE ESTOY PONIENDO DE NUEVO LOS SEEDS PARA SER CONSTANTES\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "torch.manual_seed(seed_)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_)\n",
    "    torch.cuda.manual_seed_all(seed_)  # Multi-GPU.\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "############\n",
    " \n",
    "model = CustomModelLSTM()\n",
    "# Optimizador y perdida\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "# Observen como podemos también definir un DataLoader de forma snecilla\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=False, batch_size=8)\n",
    " \n",
    "\n",
    "# Perdidas\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "# Iteramos sobre cada epoca\n",
    "for epoch in range(n_epochs):\n",
    "    # Colocamos el modelo en modo de entrenamiento\n",
    "    model.train()\n",
    "    \n",
    "    # Cargamos los batches\n",
    "    for X_batch, y_batch in loader:\n",
    "        # Obtenemos una primera prediccion\n",
    "        y_pred = model(X_batch)\n",
    "        # Calculamos la perdida\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        # Reseteamos la gradiente a cero\n",
    "        #   sino la gradiente de previas iteraciones se acumulará con las nuevas\n",
    "        optimizer.zero_grad()\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        # Aplicar las gradientes para actualizar los parametros del modelo\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validación cada 100 epocas\n",
    "    if epoch % 100 != 0 and epoch != n_epochs-1:\n",
    "        continue\n",
    "    # Colocamos el modelo en modo de evaluación\n",
    "    model.eval()\n",
    "    \n",
    "    # Deshabilitamos el calculo de gradientes\n",
    "    with torch.no_grad():\n",
    "        # Prediccion\n",
    "        y_pred = model(X_train)\n",
    "        # Calculo del RMSE - Root Mean Square Error\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        # Prediccion sobre validation\n",
    "        y_pred = model(X_test)\n",
    "        # Calculo del RMSE para validation\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "        loss_train.append(train_rmse)\n",
    "        loss_test.append(test_rmse)\n",
    "        \n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8794e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:36:19.885458Z",
     "start_time": "2023-08-05T23:36:19.729245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualización del rendimiento\n",
    "epoch = np.arange(len(loss_train))\n",
    "plt.figure()\n",
    "plt.plot(epoch, loss_train, 'r', label='Training',)\n",
    "plt.plot(epoch, loss_test, 'b', label='Test')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4eae30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:36:20.041672Z",
     "start_time": "2023-08-05T23:36:19.885458Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52fe33653ffb1624968f4a4a8b8dd877",
     "grade": false,
     "grade_id": "cell-5a5264aa04158cad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Graficamos\n",
    "with torch.no_grad():\n",
    "    # Movemos las predicciones de train para graficar\n",
    "    train_plot = np.ones_like(time_series) * np.nan\n",
    "    # Prediccion de train\n",
    "    y_pred = model(X_train)\n",
    "    # Extraemos los datos solo del ultimo paso\n",
    "    y_pred = y_pred[:, -1, :]\n",
    "    train_plot[lb : num_train] = model(X_train)[:, -1, :]\n",
    "    # Movemos las predicciones de test\n",
    "    test_plot = np.ones_like(time_series) * np.nan\n",
    "    test_plot[num_train + lb : len(time_series)] = model(X_test)[:, -1, :]\n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(time_series, label=\"Serie Original\")\n",
    "plt.plot(train_plot, c='r', label=\"Serie Train\")\n",
    "plt.plot(test_plot, c='g', label=\"Serie Test\")\n",
    "plt.xlabel('Paso en el Tiempo'), plt.ylabel('Pasajeros')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ff4a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "150fbfe9209ee5b1fc82c08094ee43fd",
     "grade": false,
     "grade_id": "cell-7a20e9d17f776c79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Nota:** Lo que se estará evaluando es el RMSE tanto en training como en test. Se evaluará que en training sea **menor a 22**, mientras que en testing sea **menor a 70**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac2320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:36:20.056981Z",
     "start_time": "2023-08-05T23:36:20.043017Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04af852d7a882ae7a5dddcd4fe42d22b",
     "grade": true,
     "grade_id": "cell-65c8e80376d46bc1",
     "locked": true,
     "points": 28,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "float(loss_test[len(loss_test)-1])\n",
    "float(test_rmse)\n",
    "loss_train\n",
    "\n",
    "with tick.marks(7):        \n",
    "    assert loss_train[-1] < 22 \n",
    "    \n",
    "with tick.marks(7):        \n",
    "    assert train_rmse < 22 \n",
    "    \n",
    "with tick.marks(7):        \n",
    "    assert loss_test[-1] < 70 \n",
    "    \n",
    "with tick.marks(7):        \n",
    "    assert test_rmse < 70 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e00b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T23:36:20.074671Z",
     "start_time": "2023-08-05T23:36:20.057977Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fc71d80805acbbec919a3972572b7f4",
     "grade": true,
     "grade_id": "cell-a895611caee19d78",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "print(\"La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio\")\n",
    "tick.summarise_marks() # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008a216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
